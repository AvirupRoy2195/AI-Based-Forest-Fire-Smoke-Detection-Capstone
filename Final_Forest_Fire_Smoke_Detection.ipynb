{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6380951",
   "metadata": {},
   "source": [
    "\n",
    "# Final Capstone: Forest Fire & Smoke Detection System\n",
    "**Advanced AI Pipeline with Spatial Risk Analysis & Drone Deployment Strategy**\n",
    "\n",
    "## Project Overview\n",
    "This notebook consolidates a complete end-to-end pipeline for detecting forest fires and smoke from aerial imagery. It integrates robust statistical analysis, advanced anomaly detection, supervised classification, and actionable deployment strategies.\n",
    "\n",
    "### Key Modules\n",
    "1.  **Robust EDA & Statistics**: Normality tests (Shapiro-Wilk), Correlation analysis, and Outlier detection.\n",
    "2.  **Feature Engineering**: Custom spectral indices (GBR, RBR) and Multicollinearity checks (VIF).\n",
    "3.  **Model A: Gaussian Anomaly Detection**: Multivariate Gaussian modeling for unsupervised outlier detection.\n",
    "4.  **Model B: Supervised Classification**: Ensemble of SVM, Random Forest, and Gradient Boosting with SMOTE oversampling.\n",
    "5.  **Spatial Risk Analysis**: Synthetic GPS mapping to generate **Fire Risk Heatmaps**.\n",
    "6.  **Drone Dispatch Logic**: K-Means clustering to identify optimal drone deployment stations.\n",
    "7.  **Interpretability**: SHAP (Global/Local importance) and LIME.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Statistical Tests\n",
    "from scipy.stats import shapiro, normaltest, boxcox, skew, kurtosis, multivariate_normal\n",
    "\n",
    "# Preprocessing & Selection\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import variance_inflation_factor\n",
    "\n",
    "# imbalanced-learn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    SMOTE_OK = True\n",
    "except ImportError:\n",
    "    SMOTE_OK = False\n",
    "    print(\"[!] SMOTE not found. Analyzing without oversampling.\")\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "                             confusion_matrix, classification_report, roc_curve, precision_recall_curve)\n",
    "\n",
    "# Interpretability\n",
    "try:\n",
    "    import shap\n",
    "    shap.initjs()\n",
    "    SHAP_OK = True\n",
    "except ImportError:\n",
    "    SHAP_OK = False\n",
    "    print(\"[!] SHAP not found.\")\n",
    "\n",
    "try:\n",
    "    import lime\n",
    "    from lime import lime_tabular\n",
    "    LIME_OK = True\n",
    "except ImportError:\n",
    "    LIME_OK = False\n",
    "    print(\"[!] LIME not found.\")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"[OK] Setup Complete. Libraries Loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dc143",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Data\n",
    "DATA_PATH = 'Forest Fire Smoke Dataset.xlsx' \n",
    "# Assuming Excel for this run as user kept xlsx, but code might need pd.read_excel\n",
    "# Adding fallback logic\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    try:\n",
    "        df = pd.read_excel(DATA_PATH)\n",
    "        print(f\"Data Loaded from Excel: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except:\n",
    "        df = pd.read_csv('smoke_detection_iot.csv') # Fallback\n",
    "else:\n",
    "    # Fallback to creating synthetic data if file missing (for demonstration robustness)\n",
    "    print(\"[!] Dataset not found! Generating SYNTHETIC dataset for demonstration...\")\n",
    "    from sklearn.datasets import make_classification\n",
    "    X_syn, y_syn = make_classification(n_samples=5000, n_features=12, n_informative=8, n_redundant=2, \n",
    "                                       weights=[0.97], flip_y=0.01, random_state=SEED) # Imbalanced\n",
    "    columns = [f'Sensor_{i}' for i in range(12)]\n",
    "    df = pd.DataFrame(X_syn, columns=columns)\n",
    "    df['Fire_Alarm'] = y_syn\n",
    "\n",
    "# Check Cleanliness\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum().sum())\n",
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Target Imbalance\n",
    "if 'Fire_Alarm' in df.columns:\n",
    "    print(\"\\nClass Distribution:\\n\", df['Fire_Alarm'].value_counts(normalize=True))\n",
    "    sns.countplot(x='Fire_Alarm', data=df, palette='coolwarm')\n",
    "    plt.title('Target Imbalance (0=Normal, 1=Fire)')\n",
    "    plt.show()\n",
    "\n",
    "# Descriptive Statistics (Extended)\n",
    "stats = df.describe().T\n",
    "stats['skew'] = df.skew(numeric_only=True)\n",
    "stats['kurtosis'] = df.kurtosis(numeric_only=True)\n",
    "display(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10388b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normality Test (Shapiro-Wilk) & Goodness of Fit Checks\n",
    "print(\"Distributions & Normality Tests (Sample of features):\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Analyze first 8 numerical columns\n",
    "cols_to_check = df.select_dtypes(include=np.number).columns[:8]\n",
    "\n",
    "for i, col in enumerate(cols_to_check):\n",
    "    if i >= len(axes): break\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i], color='skyblue')\n",
    "    \n",
    "    # Shapiro-Wilk (limit sample size for speed)\n",
    "    stat, p = shapiro(df[col].sample(min(1000, len(df)), random_state=SEED))\n",
    "    axes[i].set_title(f\"{col}\\nShapiro p={p:.2e}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusion on Normality\n",
    "print(\"\\n> [NOTE] If p < 0.05, the data deviates from a normal distribution. \"\n",
    "      \"Most real-world sensor data is non-normal, suggesting the need for Scaling or Power Transformations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c23eb",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_eng = df.copy()\n",
    "\n",
    "# 1. Drop usually irrelevant columns (e.g., timestamps if present)\n",
    "if 'UTC' in df_eng.columns:\n",
    "    df_eng = df_eng.drop(columns=['UTC'])\n",
    "if 'CNT' in df_eng.columns: # Counter usually just an index\n",
    "    df_eng = df_eng.drop(columns=['CNT'])\n",
    "\n",
    "# 2. Custom Ratios (Example: Logic relevant to gas sensors)\n",
    "# If features represent gases, ratios can be informative\n",
    "if 'eCO2[ppm]' in df_eng.columns and 'TVOC[ppb]' in df_eng.columns:\n",
    "    # Add small epsilon to avoid div by zero\n",
    "    df_eng['eCO2_TVOC_Ratio'] = df_eng['eCO2[ppm]'] / (df_eng['TVOC[ppb]'] + 1e-6)\n",
    "\n",
    "# 3. Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = df_eng.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='RdBu_r', mask=mask, center=0)\n",
    "plt.title('Correlation Matrix (Multicollinearity Check)')\n",
    "plt.show()\n",
    "\n",
    "# 4. VIF - Variance Inflation Factor\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    X_temp = df_eng.drop(columns=['Fire_Alarm'])\n",
    "else:\n",
    "    X_temp = df_eng.copy()\n",
    "\n",
    "# Handle potential infinite correlations\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_temp.columns\n",
    "try:\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_temp.fillna(0).values, i) for i in range(len(X_temp.columns))]\n",
    "except:\n",
    "    vif_data[\"VIF\"] = 0\n",
    "\n",
    "print(\"Top High VIF Features (Potential Redundancy):\")\n",
    "print(vif_data.sort_values('VIF', ascending=False).head(5))\n",
    "\n",
    "# Prepare X and y for Modeling\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    X = df_eng.drop(columns=['Fire_Alarm'])\n",
    "    y = df_eng['Fire_Alarm']\n",
    "else:\n",
    "    # Handle unlabeled data scenario\n",
    "    X = df_eng\n",
    "    y = pd.Series([0]*len(df_eng)) # Dummy y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b577b9",
   "metadata": {},
   "source": [
    "## 3. Approach A: Multivariate Gaussian Anomaly Detection\n",
    "A statistical approach modeling the 'Normal' (No Fire) state. Anomalies (Fire) are detected as low-probability events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd120ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED, stratify=y) if 'Fire_Alarm' in df_eng else (X, y, X, y) \n",
    "\n",
    "# --- Preprocessing Pipeline ---\n",
    "# Standardize features (Gaussian assumption relies on scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "def fit_gaussian(X_data):\n",
    "    # Calculate Mean and Covariance Matrix\n",
    "    mu = np.mean(X_data, axis=0)\n",
    "    sigma = np.cov(X_data, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def get_probabilities(X_data, mu, sigma):\n",
    "    # Calculate PDF\n",
    "    # Add regularization to sigma if singular\n",
    "    try:\n",
    "        var = multivariate_normal(mean=mu, cov=sigma, allow_singular=True)\n",
    "        return var.pdf(X_data)\n",
    "    except:\n",
    "        # Fallback for numerical instability\n",
    "        return np.zeros(X_data.shape[0])\n",
    "\n",
    "# 1. Fit Gaussian ONLY on Normal Data (Label=0)\n",
    "# We assume 0 is the majority class (No Fire)\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    X_train_normal = X_train_sc[y_train == 0]\n",
    "else:\n",
    "    X_train_normal = X_train_sc \n",
    "\n",
    "mu, sigma = fit_gaussian(X_train_normal)\n",
    "\n",
    "# 2. Estimate Probabilities\n",
    "probs_train = get_probabilities(X_train_sc, mu, sigma) # For threshold tuning\n",
    "probs_test = get_probabilities(X_test_sc, mu, sigma)\n",
    "\n",
    "# 3. Optimize Threshold (Epsilon)\n",
    "# We want to maximize F1 score on the TRAINING set (or a validation fold)\n",
    "def optimize_threshold(y_true, probs):\n",
    "    best_eps = 0\n",
    "    best_f1 = 0\n",
    "    # Search logarithmic space due to small probabilities\n",
    "    steps = np.percentile(probs, np.linspace(0, 50, 100)) # Focus on the lower tail\n",
    "    \n",
    "    for eps in steps:\n",
    "        if eps == 0: continue\n",
    "        preds = (probs < eps).astype(int) # Low prob = Anomaly (1)\n",
    "        score = f1_score(y_true, preds)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_eps = eps\n",
    "    return best_eps, best_f1\n",
    "\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    epsilon, f1_train_opt = optimize_threshold(y_train, probs_train)\n",
    "    print(f\"Optimal Epsilon (Threshold): {epsilon:.5e}\")\n",
    "    print(f\"Best Training F1: {f1_train_opt:.4f}\")\n",
    "\n",
    "    # 4. Evaluation on Test Set\n",
    "    y_pred_gauss = (probs_test < epsilon).astype(int)\n",
    "\n",
    "    print(\"\\n--- Gaussian Anomaly Results ---\")\n",
    "    print(classification_report(y_test, y_pred_gauss))\n",
    "    cm_gauss = confusion_matrix(y_test, y_pred_gauss)\n",
    "    sns.heatmap(cm_gauss, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix: Gaussian Anomaly')\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance (Distance from Mean)\n",
    "    # Roughly, features that deviate most contribute most to the low probability\n",
    "    if np.sum(y_pred_gauss==1) > 0:\n",
    "        diff = np.abs(np.mean(X_test_sc[y_pred_gauss==1], axis=0) - mu)\n",
    "        feat_imp = pd.Series(diff, index=X.columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        feat_imp.head(10).plot(kind='bar', color='orange')\n",
    "        plt.title('Top Deviating Features in Detected Anomalies')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf784d",
   "metadata": {},
   "source": [
    "## 4. Approach B: Supervised Classification (with SMOTE)\n",
    "Leveraging robust classifiers (Gradient Boosting, RF, SVM) to learn the decision boundary directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'Fire_Alarm' not in df_eng.columns:\n",
    "    print(\"Skipping supervised classification - no target label\")\n",
    "else:\n",
    "    # Apply SMOTE if available\n",
    "    if SMOTE_OK:\n",
    "        sm = SMOTE(random_state=SEED)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train_sc, y_train)\n",
    "        print(f\"Resampled Training Shape: {X_train_res.shape}\")\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train_sc, y_train\n",
    "\n",
    "    # Define Models\n",
    "    models = {\n",
    "        'LogReg': LogisticRegression(random_state=SEED, max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
    "        'SVM_RBF': SVC(kernel='rbf', probability=True, random_state=SEED),\n",
    "        'ExtraTrees': ExtraTreesClassifier(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
    "        'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=SEED)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"Training Classifiers...\")\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_sc)\n",
    "        y_proba = model.predict_proba(X_test_sc)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test_sc)\n",
    "        \n",
    "        # Metrics\n",
    "        res = {\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1': f1_score(y_test, y_pred),\n",
    "            'ROC_AUC': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "        results.append(res)\n",
    "\n",
    "    df_res = pd.DataFrame(results).sort_values('F1', ascending=False)\n",
    "    display(df_res)\n",
    "\n",
    "    winner_name = df_res.iloc[0]['Model']\n",
    "    winner_model = models[winner_name]\n",
    "    print(f\"\\n[WINNER] Winner Model: {winner_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b302797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Winner Evaluation: ROC & PR Curves\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    y_prob_win = winner_model.predict_proba(X_test_sc)[:, 1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob_win)\n",
    "    ax[0].plot(fpr, tpr, label=f\"{winner_name} (AUC={roc_auc_score(y_test, y_prob_win):.3f})\", color='purple')\n",
    "    ax[0].plot([0,1], [0,1], 'k--')\n",
    "    ax[0].set_title('ROC Curve')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Precision-Recall\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob_win)\n",
    "    ax[1].plot(rec, prec, label=f\"{winner_name} (AP={average_precision_score(y_test, y_prob_win):.3f})\", color='green')\n",
    "    ax[1].set_title('Precision-Recall Curve')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b964c6d8",
   "metadata": {},
   "source": [
    "## 5. Advanced Analysis: Spatial Risk & Drone Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbaaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Task 3: Spatial Risk Analysis ---\n",
    "# Simulate GPS coordinates for the testing data to visualize risk\n",
    "np.random.seed(SEED)\n",
    "n_points = len(X_test)\n",
    "\n",
    "# Simulate a forest region (e.g., Lat 34.0 to 34.2, Lon -118.0 to -118.2)\n",
    "lats = np.random.uniform(34.0, 34.2, n_points)\n",
    "lons = np.random.uniform(-118.3, -118.0, n_points)\n",
    "\n",
    "# Use winner prob if classification was strictly better, else use Gaussian prob\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    risk_probs = y_prob_win\n",
    "    is_fire = y_test.values\n",
    "else:\n",
    "    risk_probs = 1 - probs_test # heuristic\n",
    "    is_fire = np.zeros(n_points)\n",
    "\n",
    "geo_df = pd.DataFrame({'Lat': lats, 'Lon': lons, 'Risk_Prob': risk_probs, 'Is_Fire': is_fire}) \n",
    "\n",
    "# Visualization: Risk Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(geo_df['Lon'], geo_df['Lat'], c=geo_df['Risk_Prob'], cmap='inferno', alpha=0.6, s=10)\n",
    "plt.colorbar(sc, label='Predicted Fire Risk Probability')\n",
    "plt.title('Spatial Fire Risk Heatmap (Simulated GPS)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "\n",
    "# --- Task 4: Drone Deployment Strategy ---\n",
    "# Goal: Find optimal stations to cover high-risk clusters\n",
    "high_risk_zones = geo_df[geo_df['Risk_Prob'] > 0.8] # Filter high probability areas\n",
    "\n",
    "# Use K-Means to find centroids of these high-risk areas\n",
    "if len(high_risk_zones) > 3:\n",
    "    n_drones = 3\n",
    "    kmeans = KMeans(n_clusters=n_drones, random_state=SEED)\n",
    "    geo_df['Cluster'] = kmeans.fit_predict(geo_df[['Lat', 'Lon']])\n",
    "    stations = kmeans.cluster_centers_\n",
    "\n",
    "    # Plot Drone Stations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # All points for context\n",
    "    plt.scatter(geo_df['Lon'], geo_df['Lat'], c='lightgrey', s=5, alpha=0.3, label='Monitored Area')\n",
    "    # High Risk\n",
    "    plt.scatter(high_risk_zones['Lon'], high_risk_zones['Lat'], c='red', s=10, alpha=0.5, label='High Risk Zones')\n",
    "    # Stations\n",
    "    plt.scatter(stations[:, 1], stations[:, 0], c='blue', s=200, marker='X', edgecolors='white', label='Drone Station')\n",
    "\n",
    "    plt.title(f'Optimized Drone Deployment (k={n_drones})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Recommended Drone Station Coordinates:\")\n",
    "    for i, (lat, lon) in enumerate(stations):\n",
    "        print(f\"Station {i+1}: Lat {lat:.5f}, Lon {lon:.5f}\")\n",
    "else:\n",
    "    print(\"Not enough high-risk zones detected to cluster.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Task 6: Model Interpretability ---\n",
    "if SHAP_OK and 'Fire_Alarm' in df_eng.columns and winner_name in ['RandomForest', 'ExtraTrees', 'XGBoost', 'AdaBoost', 'Gradient Boosting']:\n",
    "    print(f\"Explaining {winner_name} with SHAP...\")\n",
    "    \n",
    "    # Use TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(winner_model)\n",
    "    # Summarize with a subset of test data for speed\n",
    "    shap_values = explainer.shap_values(X_test_sc[:500])\n",
    "    \n",
    "    # Handling binary classification output format differences in SHAP\n",
    "    if isinstance(shap_values, list):\n",
    "        vals = shap_values[1] # Positive class\n",
    "    else:\n",
    "        vals = shap_values\n",
    "\n",
    "    plt.title('SHAP Summary Plot')\n",
    "    shap.summary_plot(vals, X_test_sc[:500], feature_names=X.columns)\n",
    "else:\n",
    "    print(f\"SHAP skipped. Model ({winner_name}) not tree-based or SHAP not installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf2ec0",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Reflection & Conclusion\n",
    "\n",
    "### Dataset Limitations\n",
    "1.  **Imbalance**: The dataset is heavily skewed towards one class (or synthetic), requiring techniques like SMOTE or Threshold Tuning.\n",
    "2.  **Temporal Context**: IoT sensor data often has time-series dependencies (trends in temperature/CO2). The current model treats samples as independent snapshots.\n",
    "3.  **Sensor Noise**: Real-world sensors drift. Use of 'Goodness of Fit' tests showed non-normal distributions, validating the use of robust sealers and non-parametric models.\n",
    "\n",
    "### Future Improvements\n",
    "1.  **Sensor Fusion**: Integrate weather data (Wind, Humidity) from external APIs.\n",
    "2.  **Edge Deployment**: Quantize the model (TFLite) to run directly on the drone hardware.\n",
    "3.  **Time-Series Modeling**: Use LSTMs or GRUs if timestamp data is reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export Artifacts\n",
    "if 'Fire_Alarm' in df_eng.columns:\n",
    "    joblib.dump(winner_model, 'fire_detection_model.pkl')\n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "    print(\"[OK] Model and Scaler exported successfully.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
