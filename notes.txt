# Technical Skills Reference for AI-Based Forest Fire & Smoke Detection

## Project Overview
This document outlines all technical skills, methodologies, and best practices required for developing an end-to-end AI pipeline for forest fire and smoke detection from aerial imagery.

---

## Table of Contents
1. [Core Machine Learning Skills](#core-machine-learning-skills)
2. [Statistical Analysis](#statistical-analysis)
3. [Feature Engineering & Selection](#feature-engineering--selection)
4. [Model Development & Evaluation](#model-development--evaluation)
5. [Hyperparameter Tuning](#hyperparameter-tuning)
6. [Model Interpretability](#model-interpretability)
7. [Spatial Analysis & Visualization](#spatial-analysis--visualization)
8. [Version Control & Reproducibility](#version-control--reproducibility)

---

## 1. Core Machine Learning Skills

### 1.1 Python Programming
**Required Libraries:**
```python
# Data Manipulation & Analysis
import pandas as pd
import numpy as np
from scipy import stats

# Machine Learning
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Evaluation Metrics
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score,
    roc_curve, precision_recall_curve, matthews_corrcoef
)

# Feature Selection & Engineering
from sklearn.feature_selection import (
    SelectKBest, f_classif, mutual_info_classif,
    RFE, SelectFromModel, VarianceThreshold
)

# Model Interpretability
import shap
from lime import lime_tabular

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Spatial Analysis
from scipy.spatial import distance
from sklearn.cluster import DBSCAN, KMeans
```

### 1.2 Environment Setup
```bash
# Create virtual environment
python -m venv fire_detection_env
source fire_detection_env/bin/activate  # On Windows: fire_detection_env\Scripts\activate

# Install dependencies
pip install pandas numpy scipy scikit-learn xgboost lightgbm
pip install matplotlib seaborn plotly
pip install shap lime
pip install jupyter notebook ipywidgets
```

---

## 2. Statistical Analysis

### 2.1 Exploratory Data Analysis (EDA)

**Key Statistical Measures:**
- **Central Tendency:** Mean, median, mode
- **Dispersion:** Standard deviation, variance, IQR, range
- **Distribution:** Skewness, kurtosis, normality tests (Shapiro-Wilk, Kolmogorov-Smirnov)
- **Correlation:** Pearson, Spearman, Kendall coefficients

**EDA Checklist:**
```python
# 1. Dataset Overview
df.info()
df.describe()
df.shape
df.dtypes

# 2. Missing Value Analysis
missing_counts = df.isnull().sum()
missing_percentages = (df.isnull().sum() / len(df)) * 100

# 3. Target Variable Distribution
target_counts = df['target'].value_counts()
target_percentage = df['target'].value_counts(normalize=True) * 100

# 4. Statistical Tests for Feature Significance
from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu

# For continuous features vs binary target
for col in continuous_features:
    fire_samples = df[df['target'] == 1][col]
    no_fire_samples = df[df['target'] == 0][col]
    
    # T-test for normally distributed data
    stat, p_value = ttest_ind(fire_samples, no_fire_samples)
    
    # Mann-Whitney U test for non-normal data
    stat_mw, p_value_mw = mannwhitneyu(fire_samples, no_fire_samples)
```

### 2.2 Advanced Statistical Analysis

**Hypothesis Testing:**
- **Null Hypothesis (H0):** Feature has no relationship with fire/smoke presence
- **Alternative Hypothesis (H1):** Feature is significantly related to fire/smoke presence
- **Significance Level:** Î± = 0.05

**Statistical Tests:**
```python
# 1. Distribution Tests
from scipy.stats import shapiro, normaltest, anderson

def test_normality(data, feature_name):
    """Test if data follows normal distribution"""
    stat, p_value = shapiro(data)
    print(f"{feature_name}:")
    print(f"  Shapiro-Wilk test: statistic={stat:.4f}, p-value={p_value:.4f}")
    if p_value > 0.05:
        print("  Distribution appears normal (fail to reject H0)")
    else:
        print("  Distribution is not normal (reject H0)")

# 2. Correlation Analysis with Statistical Significance
from scipy.stats import pearsonr, spearmanr

def correlation_with_significance(df, features, target):
    """Calculate correlation with p-values"""
    results = []
    for feature in features:
        # Pearson correlation
        pearson_corr, pearson_p = pearsonr(df[feature], df[target])
        # Spearman correlation (for non-linear relationships)
        spearman_corr, spearman_p = spearmanr(df[feature], df[target])
        
        results.append({
            'feature': feature,
            'pearson_r': pearson_corr,
            'pearson_p': pearson_p,
            'spearman_r': spearman_corr,
            'spearman_p': spearman_p
        })
    
    return pd.DataFrame(results)

# 3. Effect Size Calculation (Cohen's d)
def cohens_d(group1, group2):
    """Calculate Cohen's d for effect size"""
    n1, n2 = len(group1), len(group2)
    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))
    return (np.mean(group1) - np.mean(group2)) / pooled_std
```

### 2.3 Outlier Detection & Treatment

**Methods:**
```python
# 1. IQR Method
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]

# 2. Z-Score Method
from scipy.stats import zscore
def detect_outliers_zscore(df, column, threshold=3):
    z_scores = np.abs(zscore(df[column]))
    return df[z_scores > threshold]

# 3. Isolation Forest
from sklearn.ensemble import IsolationForest
def detect_outliers_isolation_forest(df, contamination=0.1):
    iso_forest = IsolationForest(contamination=contamination, random_state=42)
    outlier_labels = iso_forest.fit_predict(df)
    return df[outlier_labels == -1]
```

---

## 3. Feature Engineering & Selection

### 3.1 Feature Engineering Techniques

**Domain-Specific Features for Fire Detection:**
```python
# 1. Spectral Ratios (common in remote sensing)
df['red_to_nir_ratio'] = df['red_channel'] / (df['nir_channel'] + 1e-10)
df['ndvi'] = (df['nir_channel'] - df['red_channel']) / (df['nir_channel'] + df['red_channel'] + 1e-10)

# 2. Texture Aggregations
df['texture_mean'] = df[texture_columns].mean(axis=1)
df['texture_std'] = df[texture_columns].std(axis=1)
df['texture_range'] = df[texture_columns].max(axis=1) - df[texture_columns].min(axis=1)

# 3. Intensity Features
df['intensity_variance'] = df[intensity_columns].var(axis=1)
df['intensity_skewness'] = df[intensity_columns].skew(axis=1)

# 4. Polynomial Features (interaction terms)
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
poly_features = poly.fit_transform(df[selected_features])

# 5. Binning Continuous Features
df['intensity_bin'] = pd.cut(df['mean_intensity'], bins=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])

# 6. Log Transformation (for skewed features)
df['log_feature'] = np.log1p(df['skewed_feature'])

# 7. Spatial Features
df['spatial_cluster'] = KMeans(n_clusters=10).fit_predict(df[['x_coord', 'y_coord']])
```

### 3.2 Feature Selection Methods

**3.2.1 Filter Methods**
```python
# 1. Variance Threshold
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
selected_features = selector.fit_transform(X)

# 2. Univariate Statistical Tests
from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif

# F-statistic (ANOVA F-test)
selector_f = SelectKBest(score_func=f_classif, k=20)
X_selected_f = selector_f.fit_transform(X, y)
selected_features_f = X.columns[selector_f.get_support()].tolist()

# Mutual Information
selector_mi = SelectKBest(score_func=mutual_info_classif, k=20)
X_selected_mi = selector_mi.fit_transform(X, y)

# 3. Correlation-based Selection
def remove_correlated_features(df, threshold=0.9):
    """Remove highly correlated features"""
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    to_drop = [column for column in upper_triangle.columns 
               if any(upper_triangle[column] > threshold)]
    return df.drop(columns=to_drop)
```

**3.2.2 Wrapper Methods**
```python
# 1. Recursive Feature Elimination (RFE)
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

estimator = RandomForestClassifier(n_estimators=100, random_state=42)
selector_rfe = RFE(estimator=estimator, n_features_to_select=15, step=1)
selector_rfe.fit(X, y)
selected_features_rfe = X.columns[selector_rfe.support_].tolist()

# 2. Sequential Feature Selection
from sklearn.feature_selection import SequentialFeatureSelector

sfs = SequentialFeatureSelector(
    estimator=estimator,
    n_features_to_select=15,
    direction='forward',  # or 'backward'
    scoring='roc_auc',
    cv=5
)
sfs.fit(X, y)
selected_features_sfs = X.columns[sfs.get_support()].tolist()
```

**3.2.3 Embedded Methods**
```python
# 1. L1 Regularization (Lasso)
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel

lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)
selector_lasso = SelectFromModel(lasso, prefit=False)
selector_lasso.fit(X, y)
selected_features_lasso = X.columns[selector_lasso.get_support()].tolist()

# 2. Tree-based Feature Importance
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X, y)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# Select top N features
top_n = 20
selected_features_rf = feature_importance.head(top_n)['feature'].tolist()

# 3. XGBoost Feature Importance
xgb = XGBClassifier(n_estimators=200, random_state=42)
xgb.fit(X, y)
xgb_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb.feature_importances_
}).sort_values('importance', ascending=False)
```

### 3.3 Feature Extraction

**Principal Component Analysis (PCA):**
```python
from sklearn.decomposition import PCA

# Standardize features first
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=0.95)  # Retain 95% variance
X_pca = pca.fit_transform(X_scaled)

# Analyze explained variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

print(f"Number of components: {pca.n_components_}")
print(f"Total variance explained: {sum(explained_variance_ratio):.4f}")
```

---

## 4. Model Development & Evaluation

### 4.1 Data Preprocessing

**Scaling Techniques:**
```python
# 1. StandardScaler (mean=0, std=1)
from sklearn.preprocessing import StandardScaler
scaler_standard = StandardScaler()
X_standard = scaler_standard.fit_transform(X_train)

# 2. RobustScaler (robust to outliers)
from sklearn.preprocessing import RobustScaler
scaler_robust = RobustScaler()
X_robust = scaler_robust.fit_transform(X_train)

# 3. MinMaxScaler (0 to 1 range)
from sklearn.preprocessing import MinMaxScaler
scaler_minmax = MinMaxScaler()
X_minmax = scaler_minmax.fit_transform(X_train)
```

### 4.2 Cross-Validation Strategy

**Stratified K-Fold for Imbalanced Data:**
```python
from sklearn.model_selection import StratifiedKFold, cross_validate

# Define cross-validation strategy
cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Custom scoring metrics
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation
cv_results = cross_validate(
    estimator=model,
    X=X,
    y=y,
    cv=cv_strategy,
    scoring=scoring,
    return_train_score=True,
    n_jobs=-1
)

# Analyze results
for metric in scoring.keys():
    train_scores = cv_results[f'train_{metric}']
    test_scores = cv_results[f'test_{metric}']
    print(f"{metric.upper()}:")
    print(f"  Train: {train_scores.mean():.4f} (+/- {train_scores.std():.4f})")
    print(f"  Test:  {test_scores.mean():.4f} (+/- {test_scores.std():.4f})")
```

### 4.3 Model Selection & Training

**Multiple Model Comparison:**
```python
# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),
    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),
    'SVM': SVC(probability=True, random_state=42),
    'Naive Bayes': GaussianNB()
}

# Train and evaluate all models
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    
    results.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    })

results_df = pd.DataFrame(results).sort_values('F1-Score', ascending=False)
```

### 4.4 Advanced Evaluation Metrics

**Comprehensive Metrics:**
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score,
    average_precision_score, matthews_corrcoef, cohen_kappa_score,
    balanced_accuracy_score, log_loss
)

def comprehensive_evaluation(y_true, y_pred, y_pred_proba=None):
    """Calculate all relevant metrics"""
    metrics = {
        'Accuracy': accuracy_score(y_true, y_pred),
        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, zero_division=0),
        'Recall': recall_score(y_true, y_pred),
        'F1-Score': f1_score(y_true, y_pred),
        'MCC': matthews_corrcoef(y_true, y_pred),
        'Cohen Kappa': cohen_kappa_score(y_true, y_pred)
    }
    
    if y_pred_proba is not None:
        metrics.update({
            'ROC-AUC': roc_auc_score(y_true, y_pred_proba),
            'PR-AUC': average_precision_score(y_true, y_pred_proba),
            'Log Loss': log_loss(y_true, y_pred_proba)
        })
    
    return metrics

# Calculate confusion matrix components
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
print(f"True Negatives:  {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives:  {tp}")

# Specificity
specificity = tn / (tn + fp)
print(f"Specificity: {specificity:.4f}")
```

### 4.5 Threshold Optimization

```python
from sklearn.metrics import precision_recall_curve

# Find optimal threshold
precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Threshold: {optimal_threshold:.4f}")
print(f"Precision at optimal: {precisions[optimal_idx]:.4f}")
print(f"Recall at optimal: {recalls[optimal_idx]:.4f}")
print(f"F1-Score at optimal: {f1_scores[optimal_idx]:.4f}")

# Apply optimal threshold
y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)
```

---

## 5. Hyperparameter Tuning

### 5.1 Grid Search CV

```python
from sklearn.model_selection import GridSearchCV

# Random Forest example
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]
}

grid_search_rf = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid_rf,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=2,
    return_train_score=True
)

grid_search_rf.fit(X_train, y_train)

print(f"Best parameters: {grid_search_rf.best_params_}")
print(f"Best F1-Score: {grid_search_rf.best_score_:.4f}")
```

### 5.2 Randomized Search CV

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# XGBoost example
param_distributions_xgb = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 15),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'min_child_weight': randint(1, 10),
    'gamma': uniform(0, 0.5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

random_search_xgb = RandomizedSearchCV(
    estimator=XGBClassifier(random_state=42, eval_metric='logloss'),
    param_distributions=param_distributions_xgb,
    n_iter=100,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

random_search_xgb.fit(X_train, y_train)

print(f"Best parameters: {random_search_xgb.best_params_}")
print(f"Best F1-Score: {random_search_xgb.best_score_:.4f}")
```

### 5.3 Bayesian Optimization

```python
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Define search space
search_spaces_rf = {
    'n_estimators': Integer(100, 500),
    'max_depth': Integer(5, 50),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'max_features': Real(0.1, 1.0)
}

bayes_search_rf = BayesSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    search_spaces=search_spaces_rf,
    n_iter=50,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

bayes_search_rf.fit(X_train, y_train)

print(f"Best parameters: {bayes_search_rf.best_params_}")
print(f"Best F1-Score: {bayes_search_rf.best_score_:.4f}")
```

---

## 6. Model Interpretability

### 6.1 SHAP (SHapley Additive exPlanations)

```python
import shap

# Initialize SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Summary plot (global importance)
shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
plt.tight_layout()
plt.savefig('shap_summary_bar.png', dpi=300, bbox_inches='tight')
plt.show()

# Detailed summary plot
shap.summary_plot(shap_values, X_test, show=False)
plt.tight_layout()
plt.savefig('shap_summary_detailed.png', dpi=300, bbox_inches='tight')
plt.show()

# Force plot for individual prediction
sample_idx = 0
shap.force_plot(
    explainer.expected_value,
    shap_values[sample_idx],
    X_test.iloc[sample_idx],
    matplotlib=True,
    show=False
)
plt.savefig(f'shap_force_plot_{sample_idx}.png', dpi=300, bbox_inches='tight')

# Dependence plot
shap.dependence_plot("feature_name", shap_values, X_test, show=False)
plt.tight_layout()
plt.savefig('shap_dependence.png', dpi=300, bbox_inches='tight')
```

### 6.2 LIME (Local Interpretable Model-agnostic Explanations)

```python
from lime import lime_tabular

# Initialize LIME explainer
explainer_lime = lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['No Fire', 'Fire'],
    mode='classification',
    random_state=42
)

# Explain individual prediction
sample_idx = 0
explanation = explainer_lime.explain_instance(
    data_row=X_test.iloc[sample_idx].values,
    predict_fn=model.predict_proba,
    num_features=10
)

# Show explanation
explanation.show_in_notebook()
explanation.save_to_file('lime_explanation.html')

# Get feature importance
lime_importance = explanation.as_list()
print("LIME Feature Importance:")
for feature, weight in lime_importance:
    print(f"  {feature}: {weight:.4f}")
```

### 6.3 Permutation Importance

```python
from sklearn.inspection import permutation_importance

# Calculate permutation importance
perm_importance = permutation_importance(
    model, X_test, y_test,
    n_repeats=10,
    random_state=42,
    scoring='f1'
)

# Create DataFrame
perm_importance_df = pd.DataFrame({
    'feature': X_test.columns,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values('importance_mean', ascending=False)

# Visualize
plt.figure(figsize=(10, 8))
plt.barh(perm_importance_df['feature'][:15], perm_importance_df['importance_mean'][:15])
plt.xlabel('Permutation Importance')
plt.title('Top 15 Features by Permutation Importance')
plt.tight_layout()
plt.savefig('permutation_importance.png', dpi=300, bbox_inches='tight')
```

---

## 7. Spatial Analysis & Visualization

### 7.1 Risk Score Aggregation

```python
# Spatial aggregation by grid cells
def aggregate_predictions_by_grid(df, predictions, probabilities, grid_size=100):
    """Aggregate predictions spatially"""
    df_copy = df.copy()
    df_copy['prediction'] = predictions
    df_copy['risk_score'] = probabilities
    
    # Create grid cells
    df_copy['grid_x'] = (df_copy['x_coord'] // grid_size).astype(int)
    df_copy['grid_y'] = (df_copy['y_coord'] // grid_size).astype(int)
    
    # Aggregate
    grid_agg = df_copy.groupby(['grid_x', 'grid_y']).agg({
        'risk_score': ['mean', 'max', 'count'],
        'prediction': 'sum'
    }).reset_index()
    
    grid_agg.columns = ['grid_x', 'grid_y', 'avg_risk', 'max_risk', 'tile_count', 'fire_tiles']
    grid_agg['fire_proportion'] = grid_agg['fire_tiles'] / grid_agg['tile_count']
    
    return grid_agg

# Risk categorization
def categorize_risk(risk_score):
    """Categorize continuous risk scores"""
    if risk_score < 0.3:
        return 'Low'
    elif risk_score < 0.6:
        return 'Medium'
    elif risk_score < 0.8:
        return 'High'
    else:
        return 'Critical'

df_test['risk_category'] = df_test['risk_score'].apply(categorize_risk)
```

### 7.2 Heatmap Visualization

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Create heatmap
def create_risk_heatmap(grid_data, risk_column='avg_risk'):
    """Generate spatial risk heatmap"""
    pivot_table = grid_data.pivot(
        index='grid_y',
        columns='grid_x',
        values=risk_column
    )
    
    plt.figure(figsize=(16, 12))
    sns.heatmap(
        pivot_table,
        cmap='YlOrRd',
        cbar_kws={'label': 'Fire Risk Score'},
        vmin=0,
        vmax=1,
        annot=False
    )
    plt.title('Forest Fire Risk Heatmap', fontsize=16, fontweight='bold')
    plt.xlabel('Grid X Coordinate', fontsize=12)
    plt.ylabel('Grid Y Coordinate', fontsize=12)
    plt.tight_layout()
    plt.savefig('fire_risk_heatmap.png', dpi=300, bbox_inches='tight')
    plt.show()

# Interactive heatmap with Plotly
import plotly.graph_objects as go

def create_interactive_heatmap(grid_data):
    """Create interactive heatmap with Plotly"""
    pivot_table = grid_data.pivot(
        index='grid_y',
        columns='grid_x',
        values='avg_risk'
    )
    
    fig = go.Figure(data=go.Heatmap(
        z=pivot_table.values,
        x=pivot_table.columns,
        y=pivot_table.index,
        colorscale='YlOrRd',
        colorbar=dict(title='Risk Score'),
        hoverongaps=False,
        hovertemplate='Grid X: %{x}<br>Grid Y: %{y}<br>Risk: %{z:.3f}<extra></extra>'
    ))
    
    fig.update_layout(
        title='Interactive Fire Risk Heatmap',
        xaxis_title='Grid X Coordinate',
        yaxis_title='Grid Y Coordinate',
        width=1200,
        height=900
    )
    
    fig.write_html('interactive_fire_risk_heatmap.html')
    fig.show()
```

### 7.3 Spatial Clustering

```python
from sklearn.cluster import DBSCAN

# Detect high-risk clusters
def detect_fire_clusters(df, risk_threshold=0.7, eps=50, min_samples=5):
    """Identify spatial clusters of high-risk areas"""
    high_risk = df[df['risk_score'] >= risk_threshold].copy()
    
    if len(high_risk) == 0:
        return None
    
    # DBSCAN clustering
    coords = high_risk[['x_coord', 'y_coord']].values
    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(coords)
    
    high_risk['cluster'] = clustering.labels_
    
    # Analyze clusters
    cluster_stats = []
    for cluster_id in set(clustering.labels_):
        if cluster_id == -1:  # Noise points
            continue
        
        cluster_data = high_risk[high_risk['cluster'] == cluster_id]
        cluster_stats.append({
            'cluster_id': cluster_id,
            'size': len(cluster_data),
            'avg_risk': cluster_data['risk_score'].mean(),
            'max_risk': cluster_data['risk_score'].max(),
            'centroid_x': cluster_data['x_coord'].mean(),
            'centroid_y': cluster_data['y_coord'].mean()
        })
    
    return pd.DataFrame(cluster_stats).sort_values('avg_risk', ascending=False)
```

---

## 8. Version Control & Reproducibility

### 8.1 Git Best Practices

```bash
# Initialize repository
git init
git add .
git commit -m "Initial commit: Project structure"

# Branching strategy
git checkout -b feature/data-exploration
git checkout -b feature/model-development
git checkout -b feature/spatial-analysis

# Commit conventions
git commit -m "feat: Add feature engineering functions"
git commit -m "fix: Correct data scaling issue"
git commit -m "docs: Update SKILLS.md documentation"
git commit -m "refactor: Optimize heatmap generation"
```

### 8.2 Reproducibility Checklist

```python
# 1. Set random seeds
import random
import numpy as np
import tensorflow as tf

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# 2. Version tracking
import sys
print(f"Python version: {sys.version}")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"Scikit-learn version: {sklearn.__version__}")

# 3. Save model configurations
import json

model_config = {
    'model_type': 'RandomForestClassifier',
    'parameters': grid_search_rf.best_params_,
    'train_size': len(X_train),
    'test_size': len(X_test),
    'features_used': X_train.columns.tolist(),
    'scaler': 'StandardScaler',
    'random_state': SEED
}

with open('model_config.json', 'w') as f:
    json.dump(model_config, f, indent=4)

# 4. Save trained model
import joblib
joblib.dump(best_model, 'fire_detection_model.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
```

---

## Summary

This skills reference provides comprehensive guidance for:
- **Statistical rigor:** Hypothesis testing, effect sizes, distribution analysis
- **Feature engineering:** Domain-specific features, transformations, interactions
- **Feature selection:** Filter, wrapper, and embedded methods
- **Robust modeling:** Cross-validation, multiple algorithms, ensemble methods
- **Hyperparameter tuning:** Grid search, random search, Bayesian optimization
- **Model interpretability:** SHAP, LIME, permutation importance
- **Spatial analysis:** Risk aggregation, heatmaps, clustering
- **Reproducibility:** Version control, seed setting, configuration tracking

Follow these methodologies to develop a production-ready AI pipeline for forest fire detection.