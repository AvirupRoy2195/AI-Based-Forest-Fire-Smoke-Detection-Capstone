{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Robust Anomaly Detection System\n",
    "## End-to-End Workflow with Statistical Rigor\n",
    "\n",
    "This notebook provides a **complete, production-ready workflow** for anomaly detection with:\n",
    "- Multiple algorithms (Gaussian, Isolation Forest, One-Class SVM, LOF, Elliptic Envelope)\n",
    "- Advanced imbalance handling (SMOTE, ADASYN, etc.)\n",
    "- Comprehensive statistical analysis\n",
    "- Model comparison and evaluation\n",
    "- Professional visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Load and Explore Data](#data)\n",
    "3. [Statistical Analysis](#stats)\n",
    "4. [Run Complete Pipeline](#pipeline)\n",
    "5. [Model Comparison](#comparison)\n",
    "6. [Detailed Evaluation](#evaluation)\n",
    "7. [Feature Analysis](#features)\n",
    "8. [Production Deployment](#production)\n",
    "9. [Summary & Next Steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Installation <a id='setup'></a>\n",
    "\n",
    "First, let's install all required packages and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q numpy pandas scipy scikit-learn imbalanced-learn xgboost matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the robust anomaly detection system\n",
    "from robust_anomaly_detection import (\n",
    "    run_robust_anomaly_detection,\n",
    "    RobustAnomalyDetector,\n",
    "    StatisticalAnalyzer,\n",
    "    ComprehensiveEvaluator,\n",
    "    OutlierDetector\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Robust Anomaly Detection System loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Explore Data <a id='data'></a>\n",
    "\n",
    "We'll create a synthetic dataset that mimics real-world anomaly detection scenarios with class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic anomaly detection dataset\n",
    "print(\"üîß Generating synthetic dataset...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dataset parameters\n",
    "N_SAMPLES = 2000\n",
    "N_FEATURES = 25\n",
    "ANOMALY_RATIO = 0.12  # 12% anomalies (imbalanced)\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=N_SAMPLES,\n",
    "    n_features=N_FEATURES,\n",
    "    n_informative=18,\n",
    "    n_redundant=4,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=3,\n",
    "    weights=[1-ANOMALY_RATIO, ANOMALY_RATIO],\n",
    "    flip_y=0.03,\n",
    "    class_sep=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create feature names (domain-specific for context)\n",
    "feature_names = [\n",
    "    'temperature', 'humidity', 'wind_speed', 'pressure',\n",
    "    'visibility', 'precipitation', 'cloud_cover', 'uv_index',\n",
    "    'air_quality_pm25', 'air_quality_pm10', 'air_quality_o3',\n",
    "    'noise_level', 'traffic_density', 'pedestrian_count',\n",
    "    'vegetation_index', 'soil_moisture', 'solar_radiation',\n",
    "    'gas_sensor_1', 'gas_sensor_2', 'gas_sensor_3',\n",
    "    'thermal_sensor_1', 'thermal_sensor_2', 'motion_sensor',\n",
    "    'vibration_sensor', 'light_sensor'\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_series = pd.Series(y, name='target')\n",
    "\n",
    "print(f\"‚úÖ Dataset created successfully!\")\n",
    "print(f\"   Total samples: {len(X_df):,}\")\n",
    "print(f\"   Number of features: {X_df.shape[1]}\")\n",
    "print(f\"   Normal samples: {np.sum(y == 0):,} ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Anomaly samples: {np.sum(y == 1):,} ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Imbalance ratio: {np.sum(y == 0)/np.sum(y == 1):.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "display_df = X_df.copy()\n",
    "display_df['target'] = y\n",
    "\n",
    "print(\"\\nüìã Sample Data (first 10 rows):\")\n",
    "display_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "X_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Count plot\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].bar(['Normal', 'Anomaly'], counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Count', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Class Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, count in enumerate(counts):\n",
    "    axes[0].text(i, count + 20, f'{count:,}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Percentage plot\n",
    "percentages = counts / len(y) * 100\n",
    "axes[1].pie(percentages, labels=['Normal', 'Anomaly'], autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Imbalance ratio visualization\n",
    "ratio = counts[0] / counts[1]\n",
    "axes[2].barh(['Imbalance\\nRatio'], [ratio], color='#3498db', edgecolor='black', linewidth=1.5)\n",
    "axes[2].set_xlabel('Ratio (Normal:Anomaly)', fontsize=13, fontweight='bold')\n",
    "axes[2].set_title('Class Imbalance', fontsize=14, fontweight='bold')\n",
    "axes[2].text(ratio/2, 0, f'{ratio:.2f}:1', ha='center', va='center', \n",
    "            fontsize=14, fontweight='bold', color='white')\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  WARNING: Significant class imbalance detected ({ratio:.2f}:1)\")\n",
    "print(f\"   Recommendation: Use SMOTE, ADASYN, or other imbalance handling techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "print(\"\\nüìä Feature Distribution Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select 6 random features to visualize\n",
    "sample_features = np.random.choice(feature_names, 6, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(sample_features):\n",
    "    # Separate by class\n",
    "    normal_data = X_df[y == 0][feature]\n",
    "    anomaly_data = X_df[y == 1][feature]\n",
    "    \n",
    "    # Plot histograms\n",
    "    axes[idx].hist(normal_data, bins=30, alpha=0.6, label='Normal', color='#2ecc71', edgecolor='black')\n",
    "    axes[idx].hist(anomaly_data, bins=30, alpha=0.6, label='Anomaly', color='#e74c3c', edgecolor='black')\n",
    "    axes[idx].set_xlabel(feature, fontsize=11)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[idx].set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(\"\\nüîç Data Quality Check\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_counts = X_df.isnull().sum()\n",
    "print(f\"Missing values: {missing_counts.sum()}\")\n",
    "\n",
    "if missing_counts.sum() > 0:\n",
    "    print(\"\\nFeatures with missing values:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = X_df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"   Recommendation: Consider removing {duplicates} duplicate rows\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate rows detected\")\n",
    "\n",
    "print(\"\\n‚úÖ Data quality check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Statistical Analysis <a id='stats'></a>\n",
    "\n",
    "Perform comprehensive statistical analysis including:\n",
    "- Distribution testing (normality)\n",
    "- Feature significance testing\n",
    "- Effect size analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data for analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"   Training set: {len(X_train):,} samples\")\n",
    "print(f\"   Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Initialize statistical analyzer\n",
    "analyzer = StatisticalAnalyzer(alpha=0.05)\n",
    "\n",
    "# Perform comprehensive EDA\n",
    "dist_df, sig_df = analyzer.comprehensive_eda(\n",
    "    X_train.values, \n",
    "    y_train, \n",
    "    feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display distribution test results\n",
    "print(\"\\nüìä Distribution Test Results (Top 10 Non-Normal Features)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "non_normal = dist_df[~dist_df['is_normal_shapiro']].nsmallest(10, 'shapiro_p')\n",
    "print(non_normal[['feature', 'shapiro_p', 'skewness', 'kurtosis']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚úÖ Normal features: {dist_df['is_normal_shapiro'].sum()}/{len(dist_df)}\")\n",
    "print(f\"‚ö†Ô∏è  Non-normal features: {(~dist_df['is_normal_shapiro']).sum()}/{len(dist_df)}\")\n",
    "print(\"\\nüí° Recommendation: Use RobustScaler or PowerTransformer for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature significance results\n",
    "print(\"\\nüìä Feature Significance Analysis (Top 15 Most Significant)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_significant = sig_df.nsmallest(15, 'mannwhitney_p')[\n",
    "    ['feature', 'cohens_d', 'effect_size', 'mannwhitney_p', 'is_significant']\n",
    "]\n",
    "print(top_significant.to_string(index=False))\n",
    "\n",
    "# Count significant features\n",
    "n_significant = sig_df['is_significant'].sum()\n",
    "print(f\"\\n‚úÖ Statistically significant features: {n_significant}/{len(sig_df)} (p < 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature significance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Top 15 by p-value\n",
    "top_15 = sig_df.nsmallest(15, 'mannwhitney_p')\n",
    "\n",
    "# P-values (log scale)\n",
    "axes[0].barh(range(len(top_15)), -np.log10(top_15['mannwhitney_p']), \n",
    "            color='#3498db', edgecolor='black', linewidth=1)\n",
    "axes[0].set_yticks(range(len(top_15)))\n",
    "axes[0].set_yticklabels(top_15['feature'])\n",
    "axes[0].set_xlabel('-log10(p-value)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Feature Significance (p-values)', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='Œ±=0.05')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Effect sizes (Cohen's d)\n",
    "colors_effect = ['#e74c3c' if abs(d) >= 0.8 else '#f39c12' if abs(d) >= 0.5 else '#95a5a6' \n",
    "                for d in top_15['cohens_d']]\n",
    "axes[1].barh(range(len(top_15)), np.abs(top_15['cohens_d']), \n",
    "            color=colors_effect, edgecolor='black', linewidth=1)\n",
    "axes[1].set_yticks(range(len(top_15)))\n",
    "axes[1].set_yticklabels(top_15['feature'])\n",
    "axes[1].set_xlabel(\"|Cohen's d| (Effect Size)\", fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('Feature Effect Sizes', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(0.8, color='#e74c3c', linestyle='--', linewidth=2, label='Large (0.8)')\n",
    "axes[1].axvline(0.5, color='#f39c12', linestyle='--', linewidth=2, label='Medium (0.5)')\n",
    "axes[1].axvline(0.2, color='#95a5a6', linestyle='--', linewidth=2, label='Small (0.2)')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Higher -log10(p-value) = More statistically significant\")\n",
    "print(\"   - |Cohen's d| ‚â• 0.8 = Large effect (red)\")\n",
    "print(\"   - |Cohen's d| ‚â• 0.5 = Medium effect (orange)\")\n",
    "print(\"   - |Cohen's d| ‚â• 0.2 = Small effect (gray)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Complete Pipeline <a id='pipeline'></a>\n",
    "\n",
    "Now we'll run the complete robust anomaly detection pipeline which:\n",
    "1. Preprocesses data with optimal scaling\n",
    "2. Handles class imbalance\n",
    "3. Trains multiple anomaly detection models\n",
    "4. Evaluates and compares all models\n",
    "5. Selects the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the complete robust anomaly detection pipeline\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üöÄ RUNNING COMPLETE ANOMALY DETECTION PIPELINE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "detector, results = run_robust_anomaly_detection(\n",
    "    X=X_df,\n",
    "    y=y,\n",
    "    test_size=0.25,\n",
    "    scaling_method='robust',      # Use RobustScaler (resistant to outliers)\n",
    "    imbalance_method='smote',     # Use SMOTE for class balancing\n",
    "    models='all',                 # Train all 5 available models\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Comparison <a id='comparison'></a>\n",
    "\n",
    "Compare all trained models across multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive model comparison\n",
    "comparison_df = results['comparison']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Select metrics to display\n",
    "display_metrics = ['model', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                  'balanced_accuracy', 'roc_auc', 'pr_auc', 'matthews_corrcoef']\n",
    "display_metrics = [m for m in display_metrics if m in comparison_df.columns]\n",
    "\n",
    "print(comparison_df[display_metrics].to_string(index=False))\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {results['best_model'].upper()}\")\n",
    "best_row = comparison_df[comparison_df['model'] == results['best_model']].iloc[0]\n",
    "print(f\"   F1-Score: {best_row['f1']:.4f}\")\n",
    "print(f\"   Precision: {best_row['precision']:.4f}\")\n",
    "print(f\"   Recall: {best_row['recall']:.4f}\")\n",
    "print(f\"   ROC-AUC: {best_row.get('roc_auc', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison across multiple metrics\n",
    "metrics_to_plot = ['precision', 'recall', 'f1', 'balanced_accuracy']\n",
    "models = comparison_df['model'].values\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = comparison_df[metric].values\n",
    "    \n",
    "    # Color best model differently\n",
    "    colors = ['#e74c3c' if model == results['best_model'] else '#3498db' \n",
    "             for model in models]\n",
    "    \n",
    "    bars = axes[i].barh(models, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[i].set_xlabel(metric.replace('_', ' ').title(), fontsize=13, fontweight='bold')\n",
    "    axes[i].set_title(f'{metric.replace(\"_\", \" \").title()} Comparison', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlim([0, 1])\n",
    "    axes[i].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, (v, bar) in enumerate(zip(values, bars)):\n",
    "        axes[i].text(v + 0.02, j, f'{v:.3f}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå Note: Red bar indicates the best performing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for model comparison\n",
    "from math import pi\n",
    "\n",
    "# Select top 3 models\n",
    "top_3_models = comparison_df.nlargest(3, 'f1')['model'].values\n",
    "metrics_radar = ['precision', 'recall', 'f1', 'balanced_accuracy']\n",
    "\n",
    "# Number of variables\n",
    "num_vars = len(metrics_radar)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors_radar = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "\n",
    "for idx, model in enumerate(top_3_models):\n",
    "    values = comparison_df[comparison_df['model'] == model][metrics_radar].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors_radar[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])\n",
    "\n",
    "# Fix axis labels\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics_radar], fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Comparison (Top 3)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Detailed Evaluation <a id='evaluation'></a>\n",
    "\n",
    "Detailed evaluation of the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get best model predictions\n",
    "best_model = results['best_model']\n",
    "y_pred_best = results['y_pred_best']\n",
    "y_prob_best = results['y_prob_best']\n",
    "y_test_best = results['y_test']\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"üìä DETAILED CLASSIFICATION REPORT - {best_model.upper()}\")\n",
    "print(\"=\"*100)\n",
    "print(classification_report(y_test_best, y_pred_best, \n",
    "                           target_names=['Normal', 'Anomaly'],\n",
    "                           digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_test_best, y_pred_best)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count-based confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "           xticklabels=['Normal', 'Anomaly'],\n",
    "           yticklabels=['Normal', 'Anomaly'],\n",
    "           cbar_kws={'label': 'Count'},\n",
    "           annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "axes[0].set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title(f'Confusion Matrix (Counts) - {best_model}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Percentage-based confusion matrix\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges', ax=axes[1],\n",
    "           xticklabels=['Normal', 'Anomaly'],\n",
    "           yticklabels=['Normal', 'Anomaly'],\n",
    "           cbar_kws={'label': 'Percentage (%)'},\n",
    "           annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "axes[1].set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title(f'Confusion Matrix (%) - {best_model}', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix breakdown\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nüìä Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives (TN):  {tn:,} - Correctly identified normal samples\")\n",
    "print(f\"   False Positives (FP): {fp:,} - Normal samples incorrectly flagged as anomalies\")\n",
    "print(f\"   False Negatives (FN): {fn:,} - Anomalies missed by the model\")\n",
    "print(f\"   True Positives (TP):  {tp:,} - Correctly identified anomalies\")\n",
    "print(f\"\\n   Specificity (True Negative Rate): {tn/(tn+fp):.4f}\")\n",
    "print(f\"   Sensitivity (True Positive Rate): {tp/(tp+fn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall Curves\n",
    "if y_prob_best is not None:\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve, auc, roc_auc_score, average_precision_score\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_best, y_prob_best)\n",
    "    roc_auc = roc_auc_score(y_test_best, y_prob_best)\n",
    "    \n",
    "    axes[0].plot(fpr, tpr, color='#e74c3c', lw=3,\n",
    "                label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='#95a5a6', lw=2, linestyle='--', label='Random Classifier')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_title(f'ROC Curve - {best_model}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc=\"lower right\", fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test_best, y_prob_best)\n",
    "    pr_auc = average_precision_score(y_test_best, y_prob_best)\n",
    "    \n",
    "    axes[1].plot(recall_curve, precision_curve, color='#3498db', lw=3,\n",
    "                label=f'PR curve (AUC = {pr_auc:.4f})')\n",
    "    axes[1].set_xlabel('Recall', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_title(f'Precision-Recall Curve - {best_model}', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc=\"lower left\", fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Curve Analysis:\")\n",
    "    print(f\"   ROC-AUC: {roc_auc:.4f} - Overall discrimination ability\")\n",
    "    print(f\"   PR-AUC:  {pr_auc:.4f} - Performance on imbalanced data\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Probability scores not available for this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis - examine misclassified samples\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üîç ERROR ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get indices of misclassified samples\n",
    "misclassified_idx = np.where(y_test_best != y_pred_best)[0]\n",
    "false_positives_idx = np.where((y_test_best == 0) & (y_pred_best == 1))[0]\n",
    "false_negatives_idx = np.where((y_test_best == 1) & (y_pred_best == 0))[0]\n",
    "\n",
    "print(f\"\\nTotal misclassified samples: {len(misclassified_idx)} ({len(misclassified_idx)/len(y_test_best)*100:.2f}%)\")\n",
    "print(f\"   False Positives: {len(false_positives_idx)} - Normal samples flagged as anomalies\")\n",
    "print(f\"   False Negatives: {len(false_negatives_idx)} - Anomalies that were missed\")\n",
    "\n",
    "# Cost analysis (assuming costs)\n",
    "cost_fp = 1.0  # Cost of false alarm\n",
    "cost_fn = 10.0  # Cost of missed anomaly (usually higher)\n",
    "\n",
    "total_cost = (len(false_positives_idx) * cost_fp + len(false_negatives_idx) * cost_fn)\n",
    "print(f\"\\nüí∞ Cost Analysis (FP cost=${cost_fp}, FN cost=${cost_fn}):\")\n",
    "print(f\"   Total cost: ${total_cost:.2f}\")\n",
    "print(f\"   FP cost: ${len(false_positives_idx) * cost_fp:.2f}\")\n",
    "print(f\"   FN cost: ${len(false_negatives_idx) * cost_fn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Analysis <a id='features'></a>\n",
    "\n",
    "Analyze feature importance and contribution to anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Combine statistical significance with model insights\n",
    "top_sig_features = sig_df.nsmallest(20, 'mannwhitney_p')\n",
    "\n",
    "print(\"\\nTop 20 Features by Statistical Significance:\")\n",
    "print(top_sig_features[['feature', 'cohens_d', 'effect_size', 'mannwhitney_p']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of top features\n",
    "top_10_features = sig_df.nsmallest(10, 'mannwhitney_p')['feature'].values\n",
    "corr_matrix = X_df[top_10_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "           center=0, square=True, linewidths=1,\n",
    "           cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Correlation Matrix - Top 10 Significant Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Note: High correlations (|r| > 0.8) may indicate redundant features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions for top discriminative features\n",
    "top_5_features = sig_df.nsmallest(5, 'mannwhitney_p')['feature'].values\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_5_features):\n",
    "    normal_data = X_df[y == 0][feature]\n",
    "    anomaly_data = X_df[y == 1][feature]\n",
    "    \n",
    "    # Violin plot\n",
    "    parts = axes[idx].violinplot([normal_data, anomaly_data], \n",
    "                                 positions=[0, 1],\n",
    "                                 showmeans=True, showmedians=True)\n",
    "    axes[idx].set_xticks([0, 1])\n",
    "    axes[idx].set_xticklabels(['Normal', 'Anomaly'])\n",
    "    axes[idx].set_ylabel('Value', fontsize=11)\n",
    "    axes[idx].set_title(f'{feature}\\n(Cohen\\'s d = {sig_df[sig_df[\"feature\"]==feature][\"cohens_d\"].values[0]:.3f})', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide last subplot if odd number of features\n",
    "if len(top_5_features) < 6:\n",
    "    axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Top 5 Discriminative Features - Distribution by Class', \n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Production Deployment <a id='production'></a>\n",
    "\n",
    "Prepare the model for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_training_samples': len(X_train),\n",
    "    'n_test_samples': len(X_test),\n",
    "    'features': feature_names,\n",
    "    'scaling_method': 'robust',\n",
    "    'imbalance_method': 'smote',\n",
    "    'performance': {\n",
    "        'accuracy': float(best_row['accuracy']),\n",
    "        'precision': float(best_row['precision']),\n",
    "        'recall': float(best_row['recall']),\n",
    "        'f1_score': float(best_row['f1']),\n",
    "        'roc_auc': float(best_row.get('roc_auc', 0))\n",
    "    },\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Save model and metadata\n",
    "joblib.dump(detector, 'anomaly_detector_model.pkl')\n",
    "joblib.dump(model_metadata, 'model_metadata.pkl')\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üíæ MODEL SAVED FOR PRODUCTION\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"   1. anomaly_detector_model.pkl - Complete detector object\")\n",
    "print(\"   2. model_metadata.pkl - Model metadata and performance\")\n",
    "\n",
    "print(\"\\n‚úÖ Model ready for deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate making predictions on new data\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üîÆ MAKING PREDICTIONS ON NEW DATA\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Generate some \"new\" data\n",
    "X_new, y_new_true = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=N_FEATURES,\n",
    "    n_informative=18,\n",
    "    n_redundant=4,\n",
    "    n_classes=2,\n",
    "    weights=[0.88, 0.12],\n",
    "    random_state=999\n",
    ")\n",
    "\n",
    "X_new_df = pd.DataFrame(X_new, columns=feature_names)\n",
    "\n",
    "print(f\"\\nNew data shape: {X_new_df.shape}\")\n",
    "\n",
    "# Preprocess new data using the same transformer\n",
    "X_new_proc, _ = detector.preprocess_data(X_new_df, X_new_df)\n",
    "\n",
    "# Get predictions from all models\n",
    "new_predictions, new_probabilities = detector.predict_all_models(X_new_proc)\n",
    "\n",
    "# Use best model's predictions\n",
    "final_predictions = new_predictions[best_model]\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions generated using {best_model}\")\n",
    "print(f\"   Predicted normal: {np.sum(final_predictions == 0)}\")\n",
    "print(f\"   Predicted anomalies: {np.sum(final_predictions == 1)}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = X_new_df.copy()\n",
    "results_df['prediction'] = final_predictions\n",
    "results_df['prediction_label'] = ['Anomaly' if p == 1 else 'Normal' for p in final_predictions]\n",
    "\n",
    "if best_model in new_probabilities:\n",
    "    results_df['anomaly_score'] = new_probabilities[best_model]\n",
    "\n",
    "print(\"\\nüìã Sample Predictions (first 10):\")\n",
    "display_cols = ['prediction_label'] + feature_names[:3]\n",
    "if 'anomaly_score' in results_df.columns:\n",
    "    display_cols.append('anomaly_score')\n",
    "    \n",
    "results_df[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production inference example\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üöÄ PRODUCTION INFERENCE EXAMPLE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "production_code = '''\n",
    "# Production Inference Code\n",
    "# ========================\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the saved model\n",
    "detector = joblib.load('anomaly_detector_model.pkl')\n",
    "metadata = joblib.load('model_metadata.pkl')\n",
    "\n",
    "# Load new data\n",
    "X_new = pd.read_csv('new_data.csv')\n",
    "\n",
    "# Ensure features match training\n",
    "assert list(X_new.columns) == metadata['features'], \"Feature mismatch!\"\n",
    "\n",
    "# Preprocess\n",
    "X_new_proc, _ = detector.preprocess_data(X_new, X_new)\n",
    "\n",
    "# Predict\n",
    "predictions, probabilities = detector.predict_all_models(X_new_proc)\n",
    "final_pred = predictions[metadata['model_name']]\n",
    "\n",
    "# Get anomaly scores (if available)\n",
    "if metadata['model_name'] in probabilities:\n",
    "    anomaly_scores = probabilities[metadata['model_name']]\n",
    "else:\n",
    "    anomaly_scores = None\n",
    "\n",
    "# Create output\n",
    "output_df = X_new.copy()\n",
    "output_df['is_anomaly'] = final_pred\n",
    "output_df['anomaly_label'] = ['Anomaly' if p == 1 else 'Normal' for p in final_pred]\n",
    "if anomaly_scores is not None:\n",
    "    output_df['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Save results\n",
    "output_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(f\"Predictions complete! Found {sum(final_pred)} anomalies out of {len(final_pred)} samples.\")\n",
    "'''\n",
    "\n",
    "print(production_code)\n",
    "\n",
    "print(\"\\nüí° Deployment Checklist:\")\n",
    "print(\"   ‚úÖ Model saved and can be loaded\")\n",
    "print(\"   ‚úÖ Metadata stored for validation\")\n",
    "print(\"   ‚úÖ Preprocessing pipeline included\")\n",
    "print(\"   ‚úÖ Feature names and order preserved\")\n",
    "print(\"   ‚úÖ Performance metrics documented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary & Next Steps <a id='summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä PROJECT SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüéØ Dataset:\")\n",
    "print(f\"   Total samples: {len(X_df):,}\")\n",
    "print(f\"   Features: {len(feature_names)}\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "print(f\"   Imbalance ratio: {np.sum(y == 0)/np.sum(y == 1):.2f}:1\")\n",
    "\n",
    "print(\"\\nüî¨ Statistical Analysis:\")\n",
    "print(f\"   Significant features (p<0.05): {sig_df['is_significant'].sum()}/{len(sig_df)}\")\n",
    "print(f\"   Features with large effect size: {sum(sig_df['effect_size'] == 'large')}\")\n",
    "print(f\"   Non-normal features: {(~dist_df['is_normal_shapiro']).sum()}/{len(dist_df)}\")\n",
    "\n",
    "print(\"\\nü§ñ Models Evaluated:\")\n",
    "for model in comparison_df['model'].values:\n",
    "    row = comparison_df[comparison_df['model'] == model].iloc[0]\n",
    "    marker = \"üèÜ\" if model == best_model else \"  \"\n",
    "    print(f\"   {marker} {model}: F1={row['f1']:.4f}, Precision={row['precision']:.4f}, Recall={row['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model.upper()}\")\n",
    "print(f\"   Performance Metrics:\")\n",
    "print(f\"      Accuracy: {best_row['accuracy']:.4f}\")\n",
    "print(f\"      Precision: {best_row['precision']:.4f}\")\n",
    "print(f\"      Recall: {best_row['recall']:.4f}\")\n",
    "print(f\"      F1-Score: {best_row['f1']:.4f}\")\n",
    "print(f\"      Balanced Accuracy: {best_row['balanced_accuracy']:.4f}\")\n",
    "if 'roc_auc' in best_row and best_row['roc_auc'] is not None:\n",
    "    print(f\"      ROC-AUC: {best_row['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\nüíæ Deliverables:\")\n",
    "print(\"   ‚úÖ Trained anomaly detection model\")\n",
    "print(\"   ‚úÖ Model metadata and configuration\")\n",
    "print(\"   ‚úÖ Statistical analysis results\")\n",
    "print(\"   ‚úÖ Performance visualizations\")\n",
    "print(\"   ‚úÖ Production-ready inference code\")\n",
    "\n",
    "print(\"\\nüìà Key Findings:\")\n",
    "print(f\"   1. Successfully handled {ratio:.1f}:1 class imbalance using SMOTE\")\n",
    "print(f\"   2. {sig_df['is_significant'].sum()} features show statistical significance\")\n",
    "print(f\"   3. {best_model} achieved best performance with F1={best_row['f1']:.4f}\")\n",
    "print(f\"   4. Model correctly identifies {best_row['recall']*100:.1f}% of anomalies\")\n",
    "print(f\"   5. False alarm rate: {(1-best_row.get('specificity', 0))*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. ‚úÖ Deploy model to production environment\")\n",
    "print(\"   2. üìä Monitor model performance on real data\")\n",
    "print(\"   3. üîÑ Retrain periodically with new data\")\n",
    "print(\"   4. üéØ Fine-tune threshold based on business costs\")\n",
    "print(\"   5. üìà Track false positive/negative rates\")\n",
    "print(\"   6. üîç Investigate misclassified samples for insights\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations\n",
    "print(\"\\nüí° RECOMMENDATIONS FOR YOUR USE CASE:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n1. Model Selection:\")\n",
    "if best_model == 'gaussian':\n",
    "    print(\"   ‚úì Gaussian model performed best - your data is well-suited for parametric methods\")\n",
    "    print(\"   ‚Üí Consider PowerTransformer if features are skewed\")\n",
    "elif best_model == 'isolation_forest':\n",
    "    print(\"   ‚úì Isolation Forest performed best - good for general-purpose anomaly detection\")\n",
    "    print(\"   ‚Üí Fast training and prediction, scales well\")\n",
    "elif best_model == 'one_class_svm':\n",
    "    print(\"   ‚úì One-Class SVM performed best - strong decision boundary\")\n",
    "    print(\"   ‚Üí Good for clear separation between normal and anomalous\")\n",
    "\n",
    "print(\"\\n2. Class Imbalance:\")\n",
    "if ratio > 10:\n",
    "    print(f\"   ‚ö†Ô∏è  Severe imbalance ({ratio:.1f}:1)\")\n",
    "    print(\"   ‚Üí Consider ADASYN or ensemble methods\")\n",
    "elif ratio > 5:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate imbalance ({ratio:.1f}:1)\")\n",
    "    print(\"   ‚Üí SMOTE is working well, continue using it\")\n",
    "else:\n",
    "    print(f\"   ‚úì Manageable imbalance ({ratio:.1f}:1)\")\n",
    "    print(\"   ‚Üí Current approach is appropriate\")\n",
    "\n",
    "print(\"\\n3. Feature Engineering:\")\n",
    "n_significant = sig_df['is_significant'].sum()\n",
    "if n_significant < len(feature_names) * 0.5:\n",
    "    print(f\"   ‚ö†Ô∏è  Only {n_significant}/{len(feature_names)} features are significant\")\n",
    "    print(\"   ‚Üí Consider feature selection to remove non-significant features\")\n",
    "else:\n",
    "    print(f\"   ‚úì Good feature quality ({n_significant}/{len(feature_names)} significant)\")\n",
    "\n",
    "print(\"\\n4. Performance Optimization:\")\n",
    "if best_row['recall'] < 0.8:\n",
    "    print(\"   ‚ö†Ô∏è  Low recall - missing some anomalies\")\n",
    "    print(\"   ‚Üí Adjust decision threshold to increase sensitivity\")\n",
    "    print(\"   ‚Üí Consider cost of missing anomalies vs false alarms\")\n",
    "if best_row['precision'] < 0.8:\n",
    "    print(\"   ‚ö†Ô∏è  Low precision - many false alarms\")\n",
    "    print(\"   ‚Üí Adjust decision threshold to reduce false positives\")\n",
    "    print(\"   ‚Üí Consider SMOTE-Tomek for better boundary separation\")\n",
    "\n",
    "print(\"\\n5. Deployment:\")\n",
    "print(\"   ‚úì Model is production-ready\")\n",
    "print(\"   ‚úì Use provided inference code for deployment\")\n",
    "print(\"   ‚Üí Set up monitoring for model drift\")\n",
    "print(\"   ‚Üí Establish retraining schedule (e.g., monthly)\")\n",
    "print(\"   ‚Üí Track performance metrics over time\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully completed the **Robust Anomaly Detection System** workflow!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ **Statistical Analysis**: Comprehensive hypothesis testing and effect size analysis  \n",
    "‚úÖ **Multiple Algorithms**: Trained and compared 5 different anomaly detection models  \n",
    "‚úÖ **Imbalance Handling**: Applied SMOTE to balance the dataset  \n",
    "‚úÖ **Model Evaluation**: Used 15+ metrics for thorough evaluation  \n",
    "‚úÖ **Feature Analysis**: Identified most important and significant features  \n",
    "‚úÖ **Production Ready**: Saved model and created deployment code  \n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- `anomaly_detector_model.pkl` - Trained model ready for deployment\n",
    "- `model_metadata.pkl` - Model configuration and performance metrics\n",
    "- Various visualization plots in the notebook\n",
    "\n",
    "### Ready to Use With Your Data:\n",
    "\n",
    "Simply replace the data generation code with your own data loading code:\n",
    "\n",
    "```python\n",
    "# Load your data\n",
    "X_df = pd.read_csv('your_data.csv')\n",
    "y = pd.read_csv('your_labels.csv').values.ravel()\n",
    "\n",
    "# Run the pipeline\n",
    "detector, results = run_robust_anomaly_detection(X_df, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**For questions or support, refer to:**\n",
    "- README.md - Comprehensive documentation\n",
    "- QUICK_REFERENCE.md - Common patterns and solutions\n",
    "- PROJECT_OVERVIEW.md - Detailed project information\n",
    "\n",
    "**Happy Anomaly Detecting! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
