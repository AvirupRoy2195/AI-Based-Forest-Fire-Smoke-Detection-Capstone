{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header-cell",
            "metadata": {},
            "source": [
                "# ðŸ”¥ AI-Based Forest Fire & Smoke Detection\n",
                "\n",
                "---\n",
                "\n",
                "## Project Overview\n",
                "\n",
                "This notebook implements a **comprehensive machine learning pipeline** for forest fire and smoke detection.\n",
                "\n",
                "**Two Approaches for Handling Class Imbalance:**\n",
                "1. **Without SMOTE**: Gaussian/Multivariate Normal anomaly detection\n",
                "2. **With SMOTE**: Traditional classifiers (RF, GB, AdaBoost, etc.)\n",
                "\n",
                "**Author:** Avirup Roy | **Date:** February 2026 | **Version:** 2.1.0\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports-header",
            "metadata": {},
            "source": [
                "## 1. Environment Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard Library\n",
                "import random, sys, json, warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Data Manipulation\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy import stats\n",
                "from scipy.stats import ttest_ind, mannwhitneyu, shapiro, multivariate_normal\n",
                "\n",
                "# Machine Learning\n",
                "import sklearn\n",
                "from sklearn.model_selection import (train_test_split, StratifiedKFold, cross_val_score,\n",
                "    cross_validate, GridSearchCV, RandomizedSearchCV)\n",
                "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
                "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
                "    AdaBoostClassifier, ExtraTreesClassifier, IsolationForest)\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC, OneClassSVM\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\n",
                "from sklearn.covariance import EllipticEnvelope\n",
                "\n",
                "# Optional imports\n",
                "try:\n",
                "    from xgboost import XGBClassifier\n",
                "except: XGBClassifier = None\n",
                "try:\n",
                "    from lightgbm import LGBMClassifier\n",
                "except: LGBMClassifier = None\n",
                "try:\n",
                "    from imblearn.over_sampling import SMOTE\n",
                "    from imblearn.combine import SMOTETomek\n",
                "    SMOTE_AVAILABLE = True\n",
                "except: SMOTE_AVAILABLE = False\n",
                "try:\n",
                "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "    VIF_AVAILABLE = True\n",
                "except: VIF_AVAILABLE = False\n",
                "\n",
                "# Evaluation Metrics\n",
                "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
                "    confusion_matrix, classification_report, roc_auc_score, roc_curve, \n",
                "    precision_recall_curve, average_precision_score, matthews_corrcoef,\n",
                "    balanced_accuracy_score)\n",
                "\n",
                "# Feature Selection\n",
                "from sklearn.feature_selection import (SelectKBest, f_classif, mutual_info_classif,\n",
                "    RFE, SelectFromModel, VarianceThreshold)\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "\n",
                "# Reproducibility\n",
                "SEED = 42\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "pd.set_option('display.max_columns', 50)\n",
                "try: plt.style.use('seaborn-v0_8-whitegrid')\n",
                "except: \n",
                "    try: plt.style.use('seaborn-whitegrid')\n",
                "    except: pass\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"ENVIRONMENT READY\")\n",
                "print(f\"Python: {sys.version.split()[0]} | Scikit-learn: {sklearn.__version__}\")\n",
                "print(f\"SMOTE Available: {SMOTE_AVAILABLE}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-header",
            "metadata": {},
            "source": [
                "## 2. Data Loading & Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-loading",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Dataset\n",
                "df = pd.read_excel('Forest Fire Smoke Dataset.xlsx')\n",
                "print(f\"âœ… Dataset loaded: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "\n",
                "# Class Distribution\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"CLASS DISTRIBUTION\")\n",
                "print(\"=\" * 60)\n",
                "class_counts = df['fire_label'].value_counts()\n",
                "class_pct = df['fire_label'].value_counts(normalize=True) * 100\n",
                "imbalance_ratio = class_counts[0] / class_counts[1]\n",
                "print(f\"Class 0 (No Fire): {class_counts[0]} ({class_pct[0]:.2f}%)\")\n",
                "print(f\"Class 1 (Fire):    {class_counts[1]} ({class_pct[1]:.2f}%)\")\n",
                "print(f\"Imbalance Ratio:   {imbalance_ratio:.2f}:1\")\n",
                "\n",
                "# Check if significantly imbalanced (> 2:1)\n",
                "IS_IMBALANCED = imbalance_ratio > 2\n",
                "print(f\"\\nâš ï¸  Significantly Imbalanced: {IS_IMBALANCED}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "descriptive-stats",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive Descriptive Statistics\n",
                "print(\"=\" * 60)\n",
                "print(\"DESCRIPTIVE STATISTICS\")\n",
                "print(\"=\" * 60)\n",
                "desc_stats = df.describe().T\n",
                "desc_stats['skewness'] = df.skew()\n",
                "desc_stats['kurtosis'] = df.kurtosis()\n",
                "desc_stats['missing'] = df.isnull().sum()\n",
                "desc_stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz-distributions",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution Visualization\n",
                "features = df.columns.drop('fire_label').tolist()\n",
                "n_cols = 3\n",
                "n_rows = (len(features) + n_cols - 1) // n_cols\n",
                "\n",
                "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 3))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(features):\n",
                "    sns.histplot(data=df, x=col, hue='fire_label', kde=True, ax=axes[i], palette='coolwarm')\n",
                "    axes[i].set_title(f'{col}', fontsize=10)\n",
                "    axes[i].legend(['No Fire', 'Fire'], fontsize=8)\n",
                "\n",
                "for j in range(i+1, len(axes)):\n",
                "    axes[j].set_visible(False)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Feature Distributions by Class', y=1.02, fontsize=14, fontweight='bold')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "correlation-header",
            "metadata": {},
            "source": [
                "## 3. Correlation & Multicollinearity Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "correlation-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation Matrix\n",
                "plt.figure(figsize=(12, 10))\n",
                "corr_matrix = df.corr()\n",
                "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
                "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
                "            center=0, square=True, linewidths=0.5)\n",
                "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# High Correlations\n",
                "print(\"\\nHIGH CORRELATIONS (|r| > 0.7):\")\n",
                "high_corr = []\n",
                "for i in range(len(corr_matrix.columns)):\n",
                "    for j in range(i+1, len(corr_matrix.columns)):\n",
                "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
                "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
                "if high_corr:\n",
                "    for f1, f2, r in high_corr:\n",
                "        print(f\"  {f1} â†” {f2}: r = {r:.3f}\")\n",
                "else:\n",
                "    print(\"  No high correlations found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vif-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variance Inflation Factor (VIF) for Multicollinearity\n",
                "print(\"=\" * 60)\n",
                "print(\"VARIANCE INFLATION FACTOR (VIF) ANALYSIS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "X_features = df.drop('fire_label', axis=1)\n",
                "\n",
                "if VIF_AVAILABLE:\n",
                "    X_scaled = StandardScaler().fit_transform(X_features)\n",
                "    vif_data = pd.DataFrame()\n",
                "    vif_data['Feature'] = X_features.columns\n",
                "    vif_data['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
                "    vif_data = vif_data.sort_values('VIF', ascending=False)\n",
                "    print(vif_data.to_string(index=False))\n",
                "    print(\"\\nâš ï¸  VIF > 10 indicates high multicollinearity\")\n",
                "    \n",
                "    # Flag high VIF features\n",
                "    high_vif = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()\n",
                "    if high_vif:\n",
                "        print(f\"\\nðŸš¨ High VIF features: {high_vif}\")\n",
                "else:\n",
                "    print(\"statsmodels not available; VIF analysis skipped.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "stats-header",
            "metadata": {},
            "source": [
                "## 4. Statistical Significance Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "stat-tests",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical Tests\n",
                "print(\"=\" * 60)\n",
                "print(\"FEATURE SIGNIFICANCE TESTS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "features = df.columns.drop('fire_label').tolist()\n",
                "results_sig = []\n",
                "\n",
                "for col in features:\n",
                "    fire = df[df['fire_label'] == 1][col]\n",
                "    no_fire = df[df['fire_label'] == 0][col]\n",
                "    \n",
                "    _, p_t = ttest_ind(fire, no_fire)\n",
                "    _, p_mw = mannwhitneyu(fire, no_fire, alternative='two-sided')\n",
                "    \n",
                "    # Effect size (Cohen's d)\n",
                "    pooled_std = np.sqrt(((len(fire)-1)*fire.std()**2 + (len(no_fire)-1)*no_fire.std()**2) / (len(fire)+len(no_fire)-2))\n",
                "    cohens_d = (fire.mean() - no_fire.mean()) / pooled_std if pooled_std > 0 else 0\n",
                "    \n",
                "    results_sig.append({\n",
                "        'Feature': col,\n",
                "        'T-Test p': round(p_t, 6),\n",
                "        'Mann-Whitney p': round(p_mw, 6),\n",
                "        \"Cohen's d\": round(cohens_d, 3),\n",
                "        'Significant': p_mw < 0.05\n",
                "    })\n",
                "\n",
                "sig_df = pd.DataFrame(results_sig).sort_values(\"Cohen's d\", ascending=False, key=abs)\n",
                "print(f\"Significant features: {sig_df['Significant'].sum()}/{len(sig_df)}\")\n",
                "sig_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fe-header",
            "metadata": {},
            "source": [
                "## 5. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature-eng",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Engineering\n",
                "print(\"=\" * 60)\n",
                "print(\"FEATURE ENGINEERING\")\n",
                "print(\"=\" * 60)\n",
                "original_cols = df.shape[1]\n",
                "\n",
                "# Spectral Ratios\n",
                "df['red_green_ratio'] = df['mean_red'] / (df['mean_green'] + 1e-10)\n",
                "df['red_blue_ratio_v2'] = df['mean_red'] / (df['mean_blue'] + 1e-10)\n",
                "df['green_blue_ratio'] = df['mean_green'] / (df['mean_blue'] + 1e-10)\n",
                "\n",
                "# Color aggregations\n",
                "color_cols = ['mean_red', 'mean_green', 'mean_blue']\n",
                "df['color_mean'] = df[color_cols].mean(axis=1)\n",
                "df['color_std'] = df[color_cols].std(axis=1)\n",
                "df['color_range'] = df[color_cols].max(axis=1) - df[color_cols].min(axis=1)\n",
                "\n",
                "# Log transformations\n",
                "if 'intensity_std' in df.columns:\n",
                "    df['log_intensity_std'] = np.log1p(df['intensity_std'].clip(lower=0))\n",
                "if 'hot_pixel_fraction' in df.columns:\n",
                "    df['log_hot_pixel'] = np.log1p(df['hot_pixel_fraction'])\n",
                "\n",
                "# Interaction terms\n",
                "if 'intensity_std' in df.columns and 'hot_pixel_fraction' in df.columns:\n",
                "    df['intensity_x_hotpixel'] = df['intensity_std'] * df['hot_pixel_fraction']\n",
                "\n",
                "# Spatial coords (simulated)\n",
                "if 'x_coord' not in df.columns:\n",
                "    df['x_coord'] = np.random.randint(0, 1000, size=len(df))\n",
                "    df['y_coord'] = np.random.randint(0, 1000, size=len(df))\n",
                "\n",
                "print(f\"âœ… Features: {original_cols} â†’ {df.shape[1]} (+{df.shape[1] - original_cols} new)\")\n",
                "print(f\"New features: {list(df.columns[original_cols-1:])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fs-header",
            "metadata": {},
            "source": [
                "## 6. Feature Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature-selection",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Selection\n",
                "print(\"=\" * 60)\n",
                "print(\"FEATURE SELECTION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "X = df.drop('fire_label', axis=1)\n",
                "y = df['fire_label']\n",
                "\n",
                "# 1. Variance Threshold\n",
                "vt = VarianceThreshold(threshold=0.01)\n",
                "vt.fit(X)\n",
                "low_var = X.columns[~vt.get_support()].tolist()\n",
                "print(f\"Low variance features removed: {low_var if low_var else 'None'}\")\n",
                "\n",
                "# 2. SelectKBest\n",
                "k = min(12, X.shape[1])\n",
                "selector_kb = SelectKBest(score_func=f_classif, k=k)\n",
                "selector_kb.fit(X, y)\n",
                "scores = pd.DataFrame({'Feature': X.columns, 'F-Score': selector_kb.scores_})\n",
                "scores = scores.sort_values('F-Score', ascending=False)\n",
                "print(f\"\\nTop {k} features by F-score:\")\n",
                "print(scores.head(k).to_string(index=False))\n",
                "\n",
                "# 3. Mutual Information\n",
                "mi_scores = mutual_info_classif(X, y, random_state=SEED)\n",
                "mi_df = pd.DataFrame({'Feature': X.columns, 'MI-Score': mi_scores}).sort_values('MI-Score', ascending=False)\n",
                "print(f\"\\nTop features by Mutual Information:\")\n",
                "print(mi_df.head(8).to_string(index=False))\n",
                "\n",
                "# 4. RFE with Random Forest\n",
                "rf_est = RandomForestClassifier(n_estimators=50, random_state=SEED, n_jobs=-1)\n",
                "rfe = RFE(estimator=rf_est, n_features_to_select=10, step=1)\n",
                "rfe.fit(X, y)\n",
                "rfe_features = X.columns[rfe.support_].tolist()\n",
                "print(f\"\\nRFE Selected Features (10): {rfe_features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "approach-header",
            "metadata": {},
            "source": [
                "---\n",
                "# ðŸ”€ Dual Approach for Class Imbalance\n",
                "\n",
                "We implement two strategies:\n",
                "1. **Approach A (No SMOTE)**: Gaussian/Multivariate Normal Anomaly Detection\n",
                "2. **Approach B (With SMOTE)**: Traditional Classifiers (RF, GB, AdaBoost, etc.)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-prep",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
                "print(f\"Train class distribution: {np.bincount(y_train)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "approachA-header",
            "metadata": {},
            "source": [
                "## 7A. Approach A: Anomaly Detection (No SMOTE)\n",
                "\n",
                "Using Gaussian/Multivariate Normal models when not applying SMOTE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gaussian-anomaly",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gaussian Anomaly Detection\n",
                "print(\"=\" * 60)\n",
                "print(\"APPROACH A: GAUSSIAN ANOMALY DETECTION (No SMOTE)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Train on normal class only (No Fire = 0)\n",
                "X_train_normal = X_train_scaled[y_train == 0]\n",
                "print(f\"Training on normal class: {X_train_normal.shape[0]} samples\")\n",
                "\n",
                "# 1. Multivariate Gaussian\n",
                "print(\"\\nðŸ”¹ Multivariate Gaussian Model:\")\n",
                "mean = np.mean(X_train_normal, axis=0)\n",
                "cov = np.cov(X_train_normal, rowvar=False)\n",
                "\n",
                "# Add regularization to avoid singular matrix\n",
                "cov += np.eye(cov.shape[0]) * 1e-6\n",
                "\n",
                "try:\n",
                "    mvn = multivariate_normal(mean=mean, cov=cov, allow_singular=True)\n",
                "    log_probs = mvn.logpdf(X_test_scaled)\n",
                "    \n",
                "    # Find threshold using training data\n",
                "    train_log_probs = mvn.logpdf(X_train_scaled)\n",
                "    threshold = np.percentile(train_log_probs[y_train == 0], 5)  # 5th percentile\n",
                "    \n",
                "    y_pred_mvn = (log_probs < threshold).astype(int)\n",
                "    \n",
                "    print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_mvn):.4f}\")\n",
                "    print(f\"  Precision: {precision_score(y_test, y_pred_mvn):.4f}\")\n",
                "    print(f\"  Recall:    {recall_score(y_test, y_pred_mvn):.4f}\")\n",
                "    print(f\"  F1-Score:  {f1_score(y_test, y_pred_mvn):.4f}\")\n",
                "except Exception as e:\n",
                "    print(f\"  MVN failed: {e}\")\n",
                "\n",
                "# 2. Elliptic Envelope (Robust Gaussian)\n",
                "print(\"\\nðŸ”¹ Elliptic Envelope (Robust Covariance):\")\n",
                "ee = EllipticEnvelope(contamination=0.35, random_state=SEED)\n",
                "ee.fit(X_train_normal)\n",
                "y_pred_ee = ee.predict(X_test_scaled)\n",
                "y_pred_ee = np.where(y_pred_ee == -1, 1, 0)  # Convert: -1 (anomaly) -> 1 (fire)\n",
                "\n",
                "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_ee):.4f}\")\n",
                "print(f\"  Precision: {precision_score(y_test, y_pred_ee):.4f}\")\n",
                "print(f\"  Recall:    {recall_score(y_test, y_pred_ee):.4f}\")\n",
                "print(f\"  F1-Score:  {f1_score(y_test, y_pred_ee):.4f}\")\n",
                "\n",
                "# 3. Isolation Forest\n",
                "print(\"\\nðŸ”¹ Isolation Forest:\")\n",
                "iso_forest = IsolationForest(contamination=0.35, random_state=SEED, n_jobs=-1)\n",
                "iso_forest.fit(X_train_normal)\n",
                "y_pred_iso = iso_forest.predict(X_test_scaled)\n",
                "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)\n",
                "\n",
                "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_iso):.4f}\")\n",
                "print(f\"  Precision: {precision_score(y_test, y_pred_iso):.4f}\")\n",
                "print(f\"  Recall:    {recall_score(y_test, y_pred_iso):.4f}\")\n",
                "print(f\"  F1-Score:  {f1_score(y_test, y_pred_iso):.4f}\")\n",
                "\n",
                "# 4. One-Class SVM\n",
                "print(\"\\nðŸ”¹ One-Class SVM:\")\n",
                "ocsvm = OneClassSVM(nu=0.35, kernel='rbf', gamma='auto')\n",
                "ocsvm.fit(X_train_normal)\n",
                "y_pred_ocsvm = ocsvm.predict(X_test_scaled)\n",
                "y_pred_ocsvm = np.where(y_pred_ocsvm == -1, 1, 0)\n",
                "\n",
                "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_ocsvm):.4f}\")\n",
                "print(f\"  Precision: {precision_score(y_test, y_pred_ocsvm):.4f}\")\n",
                "print(f\"  Recall:    {recall_score(y_test, y_pred_ocsvm):.4f}\")\n",
                "print(f\"  F1-Score:  {f1_score(y_test, y_pred_ocsvm):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "anomaly-summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary - Approach A\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"APPROACH A SUMMARY: ANOMALY DETECTION MODELS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "anomaly_results = []\n",
                "for name, preds in [('Multivariate Gaussian', y_pred_mvn), \n",
                "                     ('Elliptic Envelope', y_pred_ee),\n",
                "                     ('Isolation Forest', y_pred_iso),\n",
                "                     ('One-Class SVM', y_pred_ocsvm)]:\n",
                "    anomaly_results.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': round(accuracy_score(y_test, preds), 4),\n",
                "        'Precision': round(precision_score(y_test, preds), 4),\n",
                "        'Recall': round(recall_score(y_test, preds), 4),\n",
                "        'F1-Score': round(f1_score(y_test, preds), 4),\n",
                "        'MCC': round(matthews_corrcoef(y_test, preds), 4)\n",
                "    })\n",
                "\n",
                "anomaly_df = pd.DataFrame(anomaly_results).sort_values('F1-Score', ascending=False)\n",
                "anomaly_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "approachB-header",
            "metadata": {},
            "source": [
                "## 7B. Approach B: Traditional Classifiers (With SMOTE)\n",
                "\n",
                "Using SMOTE to balance classes, then applying RF, GB, AdaBoost, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "smote-application",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply SMOTE\n",
                "print(\"=\" * 60)\n",
                "print(\"APPROACH B: SMOTE + TRADITIONAL CLASSIFIERS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "if SMOTE_AVAILABLE:\n",
                "    smote = SMOTE(random_state=SEED)\n",
                "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
                "    print(f\"âœ… SMOTE Applied\")\n",
                "    print(f\"Before: {np.bincount(y_train)} â†’ After: {np.bincount(y_train_smote)}\")\n",
                "else:\n",
                "    print(\"âš ï¸  SMOTE not available. Using class_weight='balanced' instead.\")\n",
                "    X_train_smote, y_train_smote = X_train_scaled, y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "classifiers",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Models with balanced class weights\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=SEED, class_weight='balanced'),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=SEED, class_weight='balanced', n_jobs=-1),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=SEED),\n",
                "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=SEED),\n",
                "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=SEED, class_weight='balanced', n_jobs=-1),\n",
                "    'SVM (RBF)': SVC(probability=True, random_state=SEED, class_weight='balanced'),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=SEED, class_weight='balanced'),\n",
                "    'Naive Bayes': GaussianNB()\n",
                "}\n",
                "if XGBClassifier: \n",
                "    models['XGBoost'] = XGBClassifier(n_estimators=100, random_state=SEED, eval_metric='logloss', \n",
                "                                       scale_pos_weight=imbalance_ratio, verbosity=0)\n",
                "if LGBMClassifier: \n",
                "    models['LightGBM'] = LGBMClassifier(n_estimators=100, random_state=SEED, class_weight='balanced', verbose=-1)\n",
                "\n",
                "# Cross-Validation\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"CROSS-VALIDATION RESULTS (5-Fold Stratified)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
                "cv_results = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    scores = cross_validate(model, X_train_smote, y_train_smote, cv=cv,\n",
                "                           scoring=['accuracy', 'f1', 'roc_auc', 'precision', 'recall'],\n",
                "                           return_train_score=False, n_jobs=-1)\n",
                "    cv_results.append({\n",
                "        'Model': name,\n",
                "        'CV Accuracy': f\"{scores['test_accuracy'].mean():.4f} Â± {scores['test_accuracy'].std():.4f}\",\n",
                "        'CV F1': f\"{scores['test_f1'].mean():.4f} Â± {scores['test_f1'].std():.4f}\",\n",
                "        'CV ROC-AUC': f\"{scores['test_roc_auc'].mean():.4f} Â± {scores['test_roc_auc'].std():.4f}\"\n",
                "    })\n",
                "\n",
                "cv_df = pd.DataFrame(cv_results)\n",
                "cv_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model-eval",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train & Evaluate on Test Set\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"TEST SET EVALUATION (SMOTE Models)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "results = []\n",
                "trained_models = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train_smote, y_train_smote)\n",
                "    trained_models[name] = model\n",
                "    \n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "    y_prob = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': round(accuracy_score(y_test, y_pred), 4),\n",
                "        'Precision': round(precision_score(y_test, y_pred), 4),\n",
                "        'Recall': round(recall_score(y_test, y_pred), 4),\n",
                "        'F1-Score': round(f1_score(y_test, y_pred), 4),\n",
                "        'ROC-AUC': round(roc_auc_score(y_test, y_prob), 4) if y_prob is not None else None,\n",
                "        'PR-AUC': round(average_precision_score(y_test, y_prob), 4) if y_prob is not None else None,\n",
                "        'MCC': round(matthews_corrcoef(y_test, y_pred), 4)\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values('F1-Score', ascending=False)\n",
                "results_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "roc-curves",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC & PR Curves\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# ROC Curves\n",
                "for name, model in trained_models.items():\n",
                "    if hasattr(model, 'predict_proba'):\n",
                "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
                "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "        auc = roc_auc_score(y_test, y_prob)\n",
                "        axes[0].plot(fpr, tpr, label=f'{name} ({auc:.3f})')\n",
                "\n",
                "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "axes[0].set_xlabel('False Positive Rate')\n",
                "axes[0].set_ylabel('True Positive Rate')\n",
                "axes[0].set_title('ROC Curves (SMOTE Models)', fontweight='bold')\n",
                "axes[0].legend(loc='lower right', fontsize=7)\n",
                "\n",
                "# PR Curves\n",
                "for name, model in trained_models.items():\n",
                "    if hasattr(model, 'predict_proba'):\n",
                "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
                "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
                "        pr_auc = average_precision_score(y_test, y_prob)\n",
                "        axes[1].plot(recall, precision, label=f'{name} ({pr_auc:.3f})')\n",
                "\n",
                "axes[1].set_xlabel('Recall')\n",
                "axes[1].set_ylabel('Precision')\n",
                "axes[1].set_title('Precision-Recall Curves (SMOTE Models)', fontweight='bold')\n",
                "axes[1].legend(loc='lower left', fontsize=7)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "tuning-header",
            "metadata": {},
            "source": [
                "## 8. Hyperparameter Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "hyperparameter-tuning",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter Tuning for Top Models\n",
                "print(\"=\" * 60)\n",
                "print(\"HYPERPARAMETER TUNING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Random Forest Tuning\n",
                "print(\"\\nðŸ”§ Tuning Random Forest...\")\n",
                "rf_params = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'max_depth': [10, 20, None],\n",
                "    'min_samples_split': [2, 5],\n",
                "    'min_samples_leaf': [1, 2]\n",
                "}\n",
                "rf_search = RandomizedSearchCV(RandomForestClassifier(random_state=SEED, class_weight='balanced', n_jobs=-1),\n",
                "                               rf_params, n_iter=10, cv=3, scoring='f1', random_state=SEED, n_jobs=-1)\n",
                "rf_search.fit(X_train_smote, y_train_smote)\n",
                "print(f\"Best RF params: {rf_search.best_params_}\")\n",
                "print(f\"Best RF F1 (CV): {rf_search.best_score_:.4f}\")\n",
                "\n",
                "# Gradient Boosting Tuning\n",
                "print(\"\\nðŸ”§ Tuning Gradient Boosting...\")\n",
                "gb_params = {\n",
                "    'n_estimators': [100, 150],\n",
                "    'max_depth': [3, 5, 7],\n",
                "    'learning_rate': [0.05, 0.1, 0.2]\n",
                "}\n",
                "gb_search = RandomizedSearchCV(GradientBoostingClassifier(random_state=SEED),\n",
                "                               gb_params, n_iter=10, cv=3, scoring='f1', random_state=SEED, n_jobs=-1)\n",
                "gb_search.fit(X_train_smote, y_train_smote)\n",
                "print(f\"Best GB params: {gb_search.best_params_}\")\n",
                "print(f\"Best GB F1 (CV): {gb_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "best-model-eval",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate Best Tuned Models\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"TUNED MODEL PERFORMANCE ON TEST SET\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "best_models = {\n",
                "    'Tuned RF': rf_search.best_estimator_,\n",
                "    'Tuned GB': gb_search.best_estimator_\n",
                "}\n",
                "\n",
                "for name, model in best_models.items():\n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
                "    \n",
                "    print(f\"\\n{name}:\")\n",
                "    print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
                "    print(f\"  F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
                "    print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_prob):.4f}\")\n",
                "    print(f\"  PR-AUC:    {average_precision_score(y_test, y_prob):.4f}\")\n",
                "    print(f\"\\nConfusion Matrix:\")\n",
                "    print(confusion_matrix(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-header",
            "metadata": {},
            "source": [
                "## 9. Approach Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "comparison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare Both Approaches\n",
                "print(\"=\" * 60)\n",
                "print(\"COMPARISON: APPROACH A vs APPROACH B\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Best from Approach A\n",
                "best_anomaly = anomaly_df.iloc[0]\n",
                "print(f\"\\nâœ¨ Best Anomaly Model: {best_anomaly['Model']}\")\n",
                "print(f\"   F1-Score: {best_anomaly['F1-Score']:.4f}\")\n",
                "\n",
                "# Best from Approach B\n",
                "best_classifier = results_df.iloc[0]\n",
                "print(f\"\\nâœ¨ Best SMOTE Classifier: {best_classifier['Model']}\")\n",
                "print(f\"   F1-Score: {best_classifier['F1-Score']:.4f}\")\n",
                "\n",
                "# Recommendation\n",
                "print(\"\\n\" + \"-\" * 60)\n",
                "if best_classifier['F1-Score'] > best_anomaly['F1-Score']:\n",
                "    print(\"ðŸ“Š RECOMMENDATION: Use SMOTE + Traditional Classifiers\")\n",
                "    final_model = trained_models[best_classifier['Model']]\n",
                "    final_model_name = best_classifier['Model']\n",
                "else:\n",
                "    print(\"ðŸ“Š RECOMMENDATION: Use Anomaly Detection Approach\")\n",
                "    final_model = iso_forest  # Default to Isolation Forest\n",
                "    final_model_name = 'Isolation Forest'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fi-header",
            "metadata": {},
            "source": [
                "## 10. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature-importance",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance\n",
                "best_rf = rf_search.best_estimator_\n",
                "fi = pd.DataFrame({'Feature': X.columns, 'Importance': best_rf.feature_importances_})\n",
                "fi = fi.sort_values('Importance', ascending=False)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.barplot(data=fi.head(15), x='Importance', y='Feature', palette='viridis')\n",
                "plt.title('Top 15 Features by Importance (Tuned Random Forest)', fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nTop 15 Features:\")\n",
                "print(fi.head(15).to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "spatial-header",
            "metadata": {},
            "source": [
                "## 11. Spatial Risk Heatmap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "spatial-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Spatial Risk Analysis\n",
                "def aggregate_by_grid(df, probs, grid_size=100):\n",
                "    df_copy = df.copy()\n",
                "    df_copy['risk'] = probs\n",
                "    df_copy['gx'] = (df_copy['x_coord'] // grid_size).astype(int)\n",
                "    df_copy['gy'] = (df_copy['y_coord'] // grid_size).astype(int)\n",
                "    return df_copy.groupby(['gx', 'gy'])['risk'].agg(['mean', 'max', 'count']).reset_index()\n",
                "\n",
                "probs = best_rf.predict_proba(scaler.transform(X))[:, 1]\n",
                "grid = aggregate_by_grid(df, probs)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "pivot = grid.pivot(index='gy', columns='gx', values='mean')\n",
                "sns.heatmap(pivot, cmap='YlOrRd', cbar_kws={'label': 'Risk Score'})\n",
                "plt.title('Forest Fire Risk Heatmap', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Grid X')\n",
                "plt.ylabel('Grid Y')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export-header",
            "metadata": {},
            "source": [
                "## 12. Model Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export\n",
                "joblib.dump(best_rf, 'fire_detection_model.pkl')\n",
                "joblib.dump(scaler, 'feature_scaler.pkl')\n",
                "\n",
                "config = {\n",
                "    'model': final_model_name,\n",
                "    'features': X.columns.tolist(),\n",
                "    'best_params': rf_search.best_params_,\n",
                "    'seed': SEED,\n",
                "    'smote_used': SMOTE_AVAILABLE\n",
                "}\n",
                "with open('model_config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"âœ… Model, scaler, and config exported!\")\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"PIPELINE COMPLETE!\")\n",
                "print(\"=\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}